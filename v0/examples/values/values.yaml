# -- AWS Account ID
awsAccountId: "218894879100"
# -- Override for chart name generation. When set, replaces the default chart name from Chart.yaml in resource naming
nameOverride: ""
# -- Complete override for resource naming. When set, this exact name is used for all Kubernetes resources instead of generated names
fullnameOverride: ""
# -- Number of application pod replicas to maintain for high availability and load distribution
replicaCount: 1
# -- Kubernetes deployment strategy for managing pod updates and ensuring zero-downtime deployments
strategy:
  # -- Strategy type: RollingUpdate for gradual replacement, Recreate for immediate replacement
  type: RollingUpdate
  rollingUpdate:
    # -- Container image configuration for the base-chart application
    # -- Maximum extra pods allowed during update (can be number or percentage like "25%")
    maxSurge: 1
    # -- Maximum pods that can be unavailable during update to maintain service availability
    maxUnavailable: 0
image:
  # -- List of secret names containing registry credentials for pulling images from private repositories like AWS ECR
  # Example: [{name: "aws-ecr-secret"}]
  # -- ECR Registry Name
  name: livecomments-service
  # -- Docker image repository URL
  repository: 218894879100.dkr.ecr.us-east-1.amazonaws.com
  # -- Image pull policy: IfNotPresent (cache locally), Always (always pull), Never (use local only)
  pullPolicy: Always
  # -- Specific image tag to deploy. When empty, uses the chart's appVersion from Chart.yaml
  tag: v5.12.1  # LATEST PRODUCTION VERSION
imagePullSecrets: []
serviceAccount:
  # -- Create a dedicated ServiceAccount for this application (recommended for security)
  create: true
  # -- Automatically mount the ServiceAccount token for API access (required for most applications)
  automount: true
  # -- Custom annotations for the ServiceAccount (e.g., for AWS IAM role association)
  annotations: {}
  name: ""
  # -- AWS IAM role integration for pod-level permissions via IRSA (IAM Roles for Service Accounts)
  # Note: Choose either IRSA or Pod Identity, not both
  iamRole:
    # -- AWS Pod Identity integration (newer alternative to IRSA)
    # Provides simplified IAM role association for pods without requiring OIDC configuration
    # -- Enable IAM role association for accessing AWS services like S3, RDS, etc.
    enabled: true
    # -- ARN of the IAM role to associate with this ServiceAccount
    # Example: "arn:aws:iam::218894879100:role/base-chart-pod-role"
    roleArn: ""
    # -- Specific IAM role name to associate. If empty, uses generated format: {serviceAccount}.{namespace}.pod
    name: ""
  podIdentity:
    #         resources: ["nodes"]
    #         verbs: ["get", "list"]
    # -- Enable AWS Pod Identity for IAM role association
    enabled: false
    # -- ARN of the IAM role to associate with this ServiceAccount
    # Example: "arn:aws:iam::218894879100:role/base-chart-pod-role"
    roleArn: ""
    # -- Name of the Pod Identity Association. If empty, generates from release name
    associationName: ""
    # -- EKS cluster name for Pod Identity Association (required when Pod Identity is enabled)
    clusterName: ""
    # -- Use native AWS EKS Pod Identity Association instead of Crossplane CRD
    useNativeAssociation: false
rbac:
  # -- Enable RBAC resource creation
  create: true
  # -- Role configuration for namespace-scoped permissions
  role:
    # -- Create a Role for namespace-scoped permissions
    create: true
    # -- Custom annotations for the Role
    annotations: {}
    rules: []
  clusterRole:
    # -- Create a ClusterRole for cluster-scoped permissions
    # Only enable if your application absolutely needs cluster-wide access
    create: false
    # -- Custom annotations for the ClusterRole
    annotations: {}
    rules: []
defaultAnnotations: {}
podAnnotations: {}
podLabels: {}
podSecurityContext:
  # -- Run as non-root user for security
  runAsNonRoot: true
  # -- Specific user ID to run the container (1000 is typically a safe non-root user)
  runAsUser: 1000
  # -- Specific group ID for the container
  runAsGroup: 1000
  # -- Set filesystem group ownership for volumes
  fsGroup: 1000
  # -- Ensure filesystem group ownership changes are applied to volumes
  fsGroupChangePolicy: "OnRootMismatch"
  # -- Supplemental groups for the security context
  supplementalGroups: []
  seccompProfile:
    # -- Kernel parameters (sysctls) for pod-level tuning
    # Common for network stack optimization and connection handling
    type: RuntimeDefault
  sysctls: []
securityContext:
  # -- Drop all Linux capabilities for maximum security
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  # -- Prevent running as root user
  runAsNonRoot: true
  # -- Specific user ID to run the container
  runAsUser: 1000
  # -- Specific group ID for the container
  runAsGroup: 1000
  # -- Prevent privilege escalation
  allowPrivilegeEscalation: false
  # -- Set the seccomp profile to restrict system calls
  seccompProfile:
    # - Direct memory: Off-heap buffers, typically matches heap size
    # - Safety buffer: 20% overhead for OS and container
    type: RuntimeDefault
resources:
  limits:
    # -- Maximum CPU cores (1 core = 1000m). Throttles CPU usage to prevent noisy neighbor issues
    cpu: 400m
    # -- Maximum memory including heap, non-heap, and direct memory. Prevents OOM kills
    memory: 450Mi
  requests:
    # -- Kubernetes Service configuration for network access and load balancing
    # -- Guaranteed CPU allocation for consistent performance. Used by scheduler for placement
    cpu: 25m
    # -- Guaranteed memory allocation. Should match limits for memory-intensive applications
    memory: 450Mi
service:
  # -- Short alias name used for service discovery and internal routing
  alias: base-chart
  # -- Full service name for DNS resolution within the cluster
  name: base-chart
  # -- Service Annotations
  annotations: {}
  portName: http
  # -- Network protocol: TCP for HTTP/WebSocket, UDP for streaming protocols
  protocol: TCP
  # -- Service type: ClusterIP (internal), LoadBalancer (external), NodePort (development)
  type: ClusterIP
  # -- External port exposed by the Service for client connections
  port: 80
  # -- Internal port the container listens on for incoming requests
  targetPort: 8080
  # -- NodePort for development/debugging (only when type is NodePort)
  nodePort:
    # -- Additional container ports configuration (JMX, metrics)
    containerPorts:
      # -- JMX (Java Management Extensions) port for JVM monitoring and management
      jmx:
        # -- Prometheus metrics port for application performance monitoring
        # -- Enable JMX port for Java application monitoring with tools like JConsole, VisualVM
        enabled: false
        # -- Expose JMX port through the main Service (false for security - use port-forwarding instead)
        exposeService: false
        # -- Named port identifier for JMX endpoint
        containerPortName: jmx
        # -- Port number for JMX connections (standard JMX port range)
        containerPort: 9016
        # -- Protocol for JMX communication (typically TCP)
        containerProtocol: TCP
      metrics:
        # -- Liveness probe determines if the container is healthy and should be restarted if failing
        # Used for detecting deadlocks, infinite loops, or unrecoverable application states
        # -- Enable metrics endpoint for Prometheus scraping and monitoring dashboards
        enabled: true
        # -- Expose metrics port through Service for ServiceMonitor discovery
        exposeService: true
        # -- Named port identifier for metrics endpoint
        containerPortName: metrics
        # -- Port number for Prometheus metrics scraping (/metrics endpoint)
        containerPort: 5555
        # -- Protocol for metrics communication
        containerProtocol: TCP
livenessProbe:
  enabled: true
  config:
    # -- Readiness probe determines if the container can accept traffic (added/removed from Service endpoints)
    # Used for controlling traffic flow during startup, rolling updates, and temporary unavailability
    # -- HTTP health check endpoint that returns 200 OK when application is functioning
    httpGet:
      # -- Wait time before starting health checks to allow application startup (JVM warmup, dependency connections)
      path: /healthy
      port: 8080
      scheme: HTTP
    initialDelaySeconds: 40
    # -- Number of consecutive check failures before restarting the container (prevents flapping)
    failureThreshold: 5
    # -- Frequency of health checks during normal operation
    periodSeconds: 30
    # -- Number of consecutive successes to mark container as healthy again after failure
    successThreshold: 1
    # -- Maximum time to wait for health check response before marking as failed
    timeoutSeconds: 10
readinessProbe:
  enabled: true
  config:
    # -- Startup probe protects slow-starting containers from being killed by liveness probe during initialization
    # Gives applications extended time to complete startup procedures like database migrations, cache warming
    # -- HTTP endpoint check to verify application is ready to serve requests
    httpGet:
      # -- Delay before starting readiness checks (should be less than liveness to avoid restart loops)
      path: /healthy
      port: 8080
      scheme: HTTP
    initialDelaySeconds: 60
    # -- Number of consecutive successes required to mark pod as ready for traffic
    successThreshold: 1
    # -- Number of consecutive failures before removing pod from Service load balancing
    failureThreshold: 5
    # -- Frequency of readiness checks (more frequent than liveness for responsive traffic management)
    periodSeconds: 10
    # -- Timeout for readiness check response
    timeoutSeconds: 5
startupProbe:
  enabled: true
  config:
    # -- Horizontal Pod Autoscaler for automatic scaling based on resource utilization and custom metrics
    # -- HTTP endpoint to verify application has completed startup initialization
    httpGet:
      # -- Delay before first startup check (minimal since this probe handles slow startups)
      path: /healthy
      port: 8080
      scheme: HTTP
    initialDelaySeconds: 15
    # -- Maximum startup check failures allowed (total startup time = failureThreshold * periodSeconds)
    failureThreshold: 30
    # -- Frequency of startup checks during application initialization
    periodSeconds: 10
    # -- Single success marks startup complete, enabling liveness/readiness probes
    successThreshold: 1
    # -- Timeout for startup check response
    timeoutSeconds: 5
autoscaling:
  # -- Enable automatic pod scaling to handle varying traffic loads
  enabled: false
  # -- Minimum pod count to maintain for baseline capacity and availability
  minReplicas: 1
  # -- Maximum pod count to prevent resource exhaustion and cost overruns
  maxReplicas: 1
  # -- CPU utilization threshold
  targetCPUUtilizationPercentage: 33
  # -- External metrics from Prometheus for custom scaling decisions
  externalMetrics:
    # -- Horizontal Pod Autoscaler Behavior
    # -- Use external metrics like connection count, queue depth, or response time for scaling
    enabled: false
    # -- Prometheus metric name for active connection count monitoring
    connectionMetricName: base_connections_current
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        -
          type: Percent
          value: 10
          periodSeconds: 60
        -
          type: Pods
          value: 1
          periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      #   persistentVolumeClaim:
      #     claimName: shared-data
      stabilizationWindowSeconds: 60
      policies:
        -
          type: Percent
          value: 100
          periodSeconds: 60
        -
          type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max
volumes: []
volumeMounts: []
arch: amd64
# -- Pod Disruption Budget to maintain availability during cluster maintenance and updates
podDisruptionBudget:
  # node-type: compute-optimized       # High-performance nodes
  # zone: us-east-1a                   # Specific availability zone
  # -- Enable Pod Disruption Budget
  enabled: false
  # -- Maximum pods that can be unavailable during voluntary disruptions (node drains, updates)
  maxUnavailable: 1
nodeSelector: {}
tolerations: []
affinity: {}
podAffinity: []
topologySpreadConstraints: []
envVars:
  # API_TIMEOUT: "30s"                  # External API timeout
  # CACHE_TTL: "300"                    # Cache time-to-live in seconds
  {JAVA_OPTS: '-XX:NativeMemoryTracking=summary -Xms100m -Xmx100m -Dthumbnailator.conserveMemoryWorkaround=true -Dlog4j2.formatMsgNoLookups=True'}
configMapEnvVars: {}
configMap: {}
secretsEnvVars: {}
secrets:
  # -- Argo Rollout configuration for advanced deployment strategies
  {application.conf: 
  auth.client_secret: viafoura
  livecomments.service.jwt.secret1: this is my secret
  livecomments.service.jwt.secret2: this is another secret
  livecomments.service.mysql.password: vf
  livecomments.service.mysql.ro.password: vf}
rollout:
  # -- Whether to enable Argo Rollouts
  enabled: true
  # -- Number of old ReplicaSets to retain for rollback
  revisionHistoryLimit: 10
  # -- Rollout strategy configuration
  strategy:
    # -- Blue-Green deployment strategy
    blueGreen:
      # -- Whether to enable blue-green deployments
      enabled: false
      # -- Whether to automatically promote new versions
      autoPromotionEnabled: false
      # -- Seconds to wait before auto-promotion
      autoPromotionSeconds: 30
      # -- Number of preview replicas (null uses main replica count)
      previewReplicaCount:
        # canary:
        # -- Seconds to wait before scaling down old version
        scaleDownDelaySeconds: 30
    canary:
      # -- Analysis configuration for automated rollout decisions
      # -- Whether to enable canary deployments
      enabled: true
      # -- Istio traffic routing configuration for canary
      trafficRouting:
        # istio:
        istio:
          # virtualService:
          virtualService:
            name: ""  # Will be set via tpl in rollout template
            routes:
              - primary
          destinationRule:
            # -- Routes managed by the rollout controller
            name: ""  # Will be set via tpl in rollout template
            canarySubsetName: canary
            stableSubsetName: stable
      managedRoutes: []
      steps:
        -
          setWeight: 1
        -
          pause: {}
        -
          setWeight: 5
        -
          pause: {}
        -
          setWeight: 10
        -
          pause:
            duration: 10m
        -
          setWeight: 20
        -
          pause:
            duration: 10m
        -
          setWeight: 30
        -
          pause:
            duration: 10m
        -
          setWeight: 40
        -
          pause:
            duration: 10m
        -
          setWeight: 50
        -
          pause:
            duration: 10m
        -
          setWeight: 60
        -
          pause:
            duration: 10m
        -
          setWeight: 80
        -
          pause:
            duration: 10m
        -
          setWeight: 100
        -
          pause: {}
      maxUnavailable: 0
      # -- Maximum number of surge pods during canary
      maxSurge: 10%
  analysis:
    # -- Whether to enable rollout analysis
    enabled: false
    # -- Number of analysis runs to perform
    count: 3
    # -- Number of failures before rollback
    failureLimit: 1
    # -- Interval between analysis runs
    interval: 1m
    # -- Response time threshold in milliseconds
    responseTimeThreshold: 500
    # -- Step at which to start analysis
    startingStep: 2
    # -- Success condition threshold (0.99 = 99% success rate)
    successCondition: 0.99
    # -- Prometheus configuration for metrics collection
    prometheus:
      # -- Analysis templates to use
      # -- Prometheus server address
      address: "http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090"
    templates:
      -
        # -- Domain configuration for external access through Istio service mesh
        templateName: success-rate
hosts:
  # -- Public domains accessible from the internet for client connections
  public:
    enabled: true
    # additionalProperties: true
    domains:
      # -- Internal/Private Domains not accessible from the internet for client connections
      - livecomments.example.org
  private:
    enabled: false
    # -- Internal/Private Domains not accessible from the internet for client connections
    domains: []
istio:
  # -- Whether to enable Istio service mesh
  enabled: true
  # -- Global Istio configurations
  globals:
    # -- Global annotations for Istio resources
    annotations: {}
  ambient:
    # -- Whether to enable Ambient Mode (ztunnel mesh)
    enabled: true
    # -- Namespace labels for ambient mode enrollment
    namespaceLabels:
      # -- Waypoint proxy configuration for L7 features
      istio.io/dataplane-mode: ambient
    waypoint:
      # -- Sidecar Mode configuration for envoy proxy injection
      # -- Whether to create a waypoint proxy for this service
      enabled: false
      # -- Traffic type for waypoint (service or workload)
      trafficType: service
  sidecar:
    # -- Whether to enable Sidecar Mode (envoy proxy injection)
    enabled: false
    # -- Sidecar injection configuration
    injection:
      # -- Sidecar proxy configuration
      # -- Sidecar injection mode (auto, enabled, disabled)
      mode: auto
      # -- Pod-level sidecar injection annotation
      podAnnotation: sidecar.istio.io/inject
      # -- Namespace-level sidecar injection label
      namespaceLabel: istio-injection
    proxy:
      # -- Certificate Manager integration
      # -- Custom proxy configuration
      config: {}
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          # -- Sidecar proxy image (uses Istio default if empty)
          cpu: 200m
          memory: 256Mi
      image: ""
      # -- Sidecar proxy log level
      logLevel: warning
  certManager:
    # -- Istio Gateway configuration
    # -- Whether to enable cert-manager for TLS certificates
    enabled: false
    # -- Certificate issuer to use
    issuer: "letsencrypt-prod"
  gateway:
    # -- Whether to create an Istio Gateway
    create: true
    # -- Gateway name (generated if empty)
    name: ""
    # -- Gateway annotations
    annotations: {}
    labels:
      # -- Gateway selector
      external-dns: "true"
    selector:
      # -- Private Gateway selector to exposure service for private traffic
      istio: "ingressgateway"
    private:
      selector:
        # -- Public Gateway selector to exposure service for public traffic
        app: "istio-private-gateway"
    public:
      selector:
        # -- Gateway server configurations
        app: "istio-public-gateway"
    servers:
      -
        # -- Whether this server configuration is enabled
        enabled: true
        # -- Port configurations
        ports:
          -
            # -- TLS configuration
            number: 80
            name: http
            protocol: HTTP
        tls:
          # -- Whether TLS is enabled
          enabled: true
          # -- TLS configuration options
          config:
            # -- Whether this server configuration is enabled
            # -- Whether to redirect HTTP to HTTPS
            httpsRedirect: true
      -
        enabled: true
        # -- Port configurations for HTTPS
        ports:
          -
            # -- TLS configuration for HTTPS
            number: 443
            name: https
            protocol: HTTP
        tls:
          # -- Whether TLS is enabled
          enabled: true
          # -- TLS configuration options
          config:
            # -- Whether this server configuration is enabled
            # -- TLS mode (AUTO_PASSTHROUGH, SIMPLE, etc.)
            mode: AUTO_PASSTHROUGH
      -
        enabled: false
        # -- Port configurations
        ports:
          -
            # -- TLS configuration
            number: 80
            name: tcpsocket
            protocol: HTTP
        tls:
          # -- Whether TLS is enabled
          enabled: true
          # -- TLS configuration options
          config:
            # -- Additional gateway configuration options
            # -- Whether to redirect HTTP to HTTPS
            httpsRedirect: true
    additionalConfig: {}
  virtualService:
    # -- Host for the VirtualService (defaults to service name)
    host: ""
    # -- Gateways to attach to the VirtualService
    gateways: []
    additionalRouteConfig: {}
    additionalHttp: []
  destinationRule:
    # -- Traffic policy for load balancing and connection pooling
    trafficPolicy:
      enabled: false
      config:
        connectionPool:
          tcp:
            maxConnections: 100
            connectTimeout: 30s
            tcpKeepalive:
              time: 7200s
              interval: 75s
          http:
            http1MaxPendingRequests: 64
            http2MaxRequests: 1000
            maxRequestsPerConnection: 10
            maxRetries: 3
        loadBalancer:
          simple: LEAST_CONN
        outlierDetection:
          # -- Fault injection configuration for testing
          consecutiveGatewayErrors: 5
          consecutive5xxErrors: 5
          interval: 30s
          baseEjectionTime: 30s
          maxEjectionPercent: 50
  faultInjection:
    # -- Whether to enable fault injection
    enabled: false
    # -- Fault injection configurations
    configs: {}
serviceMonitor:
  # -- Whether to enable ServiceMonitor for Prometheus scraping
  enabled: true
  # -- Namespace where ServiceMonitor should be deployed (if different from app namespace)
  namespace: ""
  # -- Prometheus release name for ServiceMonitor
  prometheusReleaseName: kube-prometheus-stack
  # -- Scraping interval for metrics
  interval: 10s
  # -- Scraping timeout for metrics (should be less than interval)
  scrapeTimeout: 5s
  # -- Metrics endpoint path
  path: /metrics
  # -- Additional labels for ServiceMonitor
  labels: {}
  namespaceSelector:
    # -- Enable any namespace selector (allows monitoring across namespaces)
    any: false
    # -- Specific namespaces to monitor (leave empty for current namespace only)
    matchNames: []
datadog:
  # -- Enable/disable Datadog monitoring
  enabled: true
  # -- Container name (should match your main container name)
  containerName: "base-chart"
  # -- Namespace prefix for metrics in Datadog
  namespace: "base-chart"
  # -- Metrics endpoint path
  metricsPath: "/metrics"
  # -- Collect all metrics (if true, ignores specific metrics list)
  collectAllMetrics: true
  # -- Timeout for metric collection
  timeout: 20
  # -- Enable health service check
  healthServiceCheck: true
  # -- Send histogram buckets
  sendHistogramsBuckets: true
  # -- Collect histogram buckets
  collectHistogramBuckets: true
  # -- Specific metrics to collect with transformations
  # Can be either strings (collect as-is) or objects (rename)
  metrics:
    -
      # Collect these metrics as-is
      "http_requests_total"
    - "http_request_duration_seconds"
    - "database_connections_active"
    -
      source: "go_memstats_alloc_bytes"
      target: "memory_allocated"
    -
      source: "process_cpu_seconds_total"
      target: "cpu_usage_seconds"
    -
      source: "base_custom_metric_total"
      target: "custom_operations"
    -
      # -- Metrics to ignore/exclude
      source: "prometheus_rule_evaluation_duration_seconds"
      target: "rule_evaluation_time"
  ignoreMetrics:
    - "go_gc_.*"
    - "go_goroutines"
    - "process_.*_bytes"
    - "prometheus_.*"
    - "up"
  metricTransformations:
    labels:
      -
        from: "handler"
        to: "endpoint"
      -
        from: "method"
        to: "http_method"
      -
        # -- Additional tags to add to all metrics
        # Note: version tag will be automatically replaced with the actual app version from Chart.yaml or image.tag
        from: "status_code"
        to: "response_code"
  tags:
    env: production
    team: backend
    service: base-chart
  logs:
    enabled: true
    source: "base-chart"
    service: "base-chart"
    # Log processing rules
    logProcessingRules:
      -
        type: "exclude_at_match"
        name: "exclude_health_checks"
        pattern: "GET /health"
      -
        # -- JMX (Java Management Extensions) integration configuration
        # Complete configuration for JMX monitoring with Datadog Agent
        type: "mask_sequences"
        name: "mask_credit_cards"
        pattern: "\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}"
        replacement: "****-****-****-****"
  jmx:
    # -- JMX connection URL (auto-generated if not provided)
    # Example: "service:jmx:rmi:///jndi/rmi://%%host%%:9016/jmxrmi"
    jmxUrl: ""
    # -- JMX authentication username
    user: ""
    # -- JMX authentication password (store in secret for production)
    password: ""
    # -- Whether this is a JMX integration (default: true when jmx integration is selected)
    isJmx: true
    # -- Collect default JMX metrics (heap, threads, GC, etc.)
    collectDefaultMetrics: true
    # -- Process name regex for JMX connection discovery
    # Example: ".*java.*base-chart.*"
    processNameRegex: ""
    # -- Custom tools.jar path for JMX tools
    tools_jar_path: ""
    # -- Custom name identifier for this JMX instance
    name: ""
    # -- Custom Java binary path
    java_bin_path: ""
    # -- JVM options for JMX connection
    # Example: "-Xms64m -Xmx128m"
    java_options: ""
    # -- SSL/TLS trust store path for secure JMX connections
    trust_store_path: ""
    # -- SSL/TLS trust store password
    trust_store_password: ""
    # -- SSL/TLS key store path for client certificates
    key_store_path: ""
    # -- SSL/TLS key store password
    key_store_password: ""
    # -- Enable SSL for RMI registry connections
    rmi_registry_ssl: false
    # -- RMI connection timeout in milliseconds
    rmi_connection_timeout: 20000
    # -- RMI client timeout in milliseconds
    rmi_client_timeout: 15000
    # -- Collect default JVM metrics (memory, GC, threads)
    collect_default_jvm_metrics: true
    # -- Enable new garbage collection metrics format
    new_gc_metrics: true
    # -- Custom service check prefix for JMX health checks
    service_check_prefix: "jmx"
    # -- Custom metric configuration for JMX beans
    # Define specific MBeans and attributes to collect
    conf:
      -
        # Example JMX metric collection configuration
        include:
          domain: "java.lang"
          type: "Memory"
          attribute:
            HeapMemoryUsage:
              alias: "jvm.heap_memory"
              metric_type: "gauge"
            NonHeapMemoryUsage:
              alias: "jvm.non_heap_memory"
              metric_type: "gauge"
      -
        include:
          domain: "java.lang"
          type: "GarbageCollector"
          name: "*"
          attribute:
            CollectionCount:
              alias: "jvm.gc.cms.count"
              metric_type: "counter"
            CollectionTime:
              alias: "jvm.gc.cms.time"
              metric_type: "counter"
      -
        include:
          domain: "java.lang"
          type: "Threading"
          attribute:
            ThreadCount:
              alias: "jvm.thread_count"
              metric_type: "gauge"
            DaemonThreadCount:
              # Application-specific MBeans (customize for your application)
              alias: "jvm.daemon_thread_count"
              metric_type: "gauge"
      -
        include:
          domain: "com.your.app"
          type: "ConnectionPool"
          attribute:
            ActiveConnections:
              alias: "app.db.active_connections"
              metric_type: "gauge"
            IdleConnections:
              # -- Custom integration configuration
              # For integrations other than openmetrics or jmx
              alias: "app.db.idle_connections"
              metric_type: "gauge"
  instanceConfig: {}
  integration: "jmx"
  # -- Custom init configuration for the integration
  # Additional configuration passed to the integration's init_config
  initConfig: {}
  apm:
    # -- NetworkPolicy configuration for defense-in-depth network security
    # Implements zero-trust networking with explicit allow rules for required communication
    enabled: false
    serviceName: "base-chart"
    environment: "production"
networkPolicy:
  # -- Enable NetworkPolicy creation for enhanced network security
  enabled: false
  # -- Deny-all baseline policy configuration
  denyAll:
    # -- Ingress traffic rules configuration
    # -- Create a deny-all NetworkPolicy as security baseline (recommended for production)
    enabled: false
  ingress:
    # -- Allow ingress from pods within the same namespace
    allowSameNamespace: true
    # -- Allow ingress from Istio Gateway for service mesh traffic
    allowIstioGateway: true
    # -- Allow ingress from monitoring namespace for metrics collection
    allowMonitoring: true
    # -- Name of the monitoring namespace (default: monitoring)
    monitoringNamespace: "monitoring"
    # -- Custom ingress rules for specific traffic requirements
    customRules: []
  egress:
    # -- Enable egress policies (when false, all egress is allowed)
    enabled: false
    # -- Allow DNS resolution (required for service discovery)
    allowDNS: true
    # -- Allow HTTPS traffic to internet (for external APIs, image pulls)
    allowHTTPS: true
    # -- Allow traffic within the same namespace
    allowSameNamespace: true
    # -- Allow access to AWS metadata service (required for AWS integrations)
    allowAWSMetadata: true
    # -- Custom egress rules for specific external services
    customRules: []
  monitoring:
    # -- Pod Security Standards configuration for compliance automation
    # Implements Kubernetes Pod Security Standards for enhanced security posture
    # -- Create dedicated NetworkPolicy for monitoring access
    enabled: true
    # -- Prometheus namespace for metrics scraping
    prometheusNamespace: "monitoring"
    # -- Datadog namespace for agent access
    datadogNamespace: "datadog"
podSecurityStandards:
  # -- Enable Pod Security Standards compliance features
  enabled: true
  # baseline: Minimally restrictive, prevents known privilege escalations
  # privileged: Unrestricted, suitable for system-level workloads
  level: "restricted"
  # -- Create PodSecurityPolicy for clusters without Pod Security Standards
  # (Kubernetes < 1.25 or clusters still using PSP)
  createPodSecurityPolicy: false
  # -- Create SecurityContextConstraints for OpenShift clusters
  createSecurityContextConstraints: false
  # -- Allow hostPath volumes (not recommended for restricted level)
  allowHostPaths: false
  # -- Allowed hostPath configurations (only if allowHostPaths is true)
  allowedHostPaths: []
  allowedHostPortRanges: []
  allowedCapabilities: []
  allowedSeccompProfiles: []
  apparmor:
    # -- Enable AppArmor profiles
    enabled: false
    # -- AppArmor profiles for containers
    profiles: {}
  compliance:
    # -- Enable compliance monitoring and reporting
    enabled: true
    # -- Compliance frameworks to validate against
    frameworks:
      - "pod-security-standards"
      - "cis-kubernetes"
      - "nist-cybersecurity"
    auditLogging:
      enabled: true
      # -- Violation severity levels to log
      levels:
        # -- Backup automation configuration for operational excellence
        # Provides multiple backup strategies including Velero and custom CronJob backups
        - "warn"
        - "error"
backup:
  # -- Enable backup automation features
  enabled: false
  # -- Velero backup configuration for full cluster backup
  velero:
    # -- Enable Velero backup scheduling
    enabled: false
    # -- Backup schedule in cron format (default: daily at 2 AM)
    schedule: "0 2 * * *"
    # -- Backup retention period (default: 30 days)
    retention: "720h"
    # -- Velero namespace (default: velero)
    namespace: "velero"
    # -- Storage location for backups
    storageLocation: "default"
    # -- Volume snapshot locations for persistent volumes
    volumeSnapshotLocations:
      - "default"
    includeCustomResources: true
    # -- Custom resources to include in backup
    customResources:
      - "rollouts.argoproj.io"
      - "virtualservices.networking.istio.io"
      - "destinationrules.networking.istio.io"
      - "gateways.networking.istio.io"
    hooks:
      # -- Custom CronJob backup for application-specific data
      enabled: true
  cronjob:
    # -- Enable CronJob-based backup
    enabled: false
    # -- Backup schedule in cron format (default: daily at 3 AM)
    schedule: "0 3 * * *"
    # -- Timezone for backup schedule
    timeZone: "UTC"
    # -- Concurrency policy for backup jobs
    concurrencyPolicy: "Forbid"
    # -- Number of successful backup jobs to retain
    successfulJobsHistoryLimit: 3
    # -- Number of failed backup jobs to retain
    failedJobsHistoryLimit: 1
    # -- Deadline for backup job to start
    startingDeadlineSeconds: 300
    # -- Number of retries for failed backup jobs
    backoffLimit: 2
    # -- Maximum time for backup job to complete
    activeDeadlineSeconds: 3600
    # -- Retention period for backups (in days)
    retentionDays: 30
    # -- Type of backup to perform
    backupType: "application-data"
    # -- Container image for backup executor
    image:
      # -- Resource limits and requests for backup job
      repository: "amazon/aws-cli"
      tag: "2.13.0"
      pullPolicy: "IfNotPresent"
    resources:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        # -- S3 configuration for backup storage
        cpu: "100m"
        memory: "128Mi"
    s3:
      # -- Additional environment variables for backup job
      # -- S3 bucket for backup storage
      bucket: ""
      # -- S3 prefix for backup objects
      prefix: "backups"
    extraEnv: {}
    volumes: []
    volumeMounts: []
    nodeSelector: {}
    tolerations: []
  monitoring:
    # -- Enable backup monitoring and metrics
    enabled: true
    # -- Prometheus metrics for backup status
    metrics:
      # -- Alerting configuration for backup failures
      # -- Enable backup metrics collection
      enabled: true
      # -- Metrics endpoint path
      path: "/metrics"
      # -- Metrics collection interval
      interval: "30s"
    alerting:
      # -- Enable backup failure alerts
      enabled: true
      # -- Alert severity levels
      severity: "warning"
      # -- Alert notification channels
      channels:
        # -- Grafana Dashboards Configuration
        - "slack"
        - "email"
grafana:
  # -- Dashboard configuration
  dashboards:
    # -- Enable Grafana dashboard ConfigMap creation
    # Only creates ConfigMap if dashboard file exists at dashboards/base-chart.json
    enabled: false
    # -- Grafana folder for dashboard organization
    folder: "Base-Chart"
    # -- Additional labels for dashboard ConfigMap discovery
    labels:
      # grafana.com/dashboard-uid: "base-chart-dashboard"
      # grafana.com/dashboard-title: "Base-Chart Service Monitoring"
      grafanaDashboard: "1"
      dashboardSource: "base-chart"
    annotations: {}
configmap:
  application.properties:
    auth.client_uuid: 0d0b1d94-4971-43ef-bdaa-8108ac1ffc55
    auth.refresh_token_backoff_multiplier: 1
    auth.refresh_token_interval_ms: 270000
    auth.refresh_token_max_attempts: 3
    auth.refresh_token_wait_duration_ms: 1000
    auth.service_uri: "https://auth/authorize_client"
    compliance.export.bucket: viafoura-gdpr-exports.misconfigured
    compliance.kafka.bootstrap: "kafka:9092"
    content_scores.kafka.bootstrap: "kafka:9092"
    customer.keys.get_retry_count: 30
    customer.keys.get_retry_wait_ms: 1000
    livecomments-trending.service.max_retries: 10
    livecomments-trending.service.mysql.db: livecomments
    livecomments-trending.service.mysql.ro.host: livecomments-mysql-trending
    livecomments-trending.service.mysql.ro.idle_timeout_ms: 60000
    livecomments-trending.service.mysql.ro.max_lifetime_ms: 600000
    livecomments-trending.service.mysql.ro.max_pool_size: 5
    livecomments-trending.service.mysql.ro.min_pool_size: 1
    livecomments-trending.service.mysql.ro.password: vf
    livecomments-trending.service.mysql.ro.user: livecomments
    livecomments.service.aws.bucket_name: origin-livecommentsmedia.env.net
    livecomments.service.cache.parent_site_cache_ttl_m: 10
    livecomments.service.cache.parent_site_max_cache_size: 1000
    livecomments.service.cache.relative_sections_cache_ttl_m: 5
    livecomments.service.cache.relative_sections_max_cache_size: 300
    livecomments.service.cache.site_group_cache_ttl_m: 5
    livecomments.service.cache.site_group_max_cache_size: 100
    livecomments.service.content_container_cache_max_size: 5000
    livecomments.service.content_container_cache_ttl_m: 5
    livecomments.service.content_container_comment_count_and_status_cache_max_size: 5000
    livecomments.service.content_container_comment_count_and_status_cache_ttl_m: 5
    livecomments.service.health-check.liveness.port: 8081
    livecomments.service.heartbeat_interval: 30000
    livecomments.service.max_content_page_size: 100
    livecomments.service.mysql.connection_timeout_ms: 10000
    livecomments.service.mysql.db: livecomments
    livecomments.service.mysql.host: livecomments-mysql
    livecomments.service.mysql.idle_timeout_ms: 60000
    livecomments.service.mysql.max_lifetime_ms: 60000
    livecomments.service.mysql.max_pool_size: 5
    livecomments.service.mysql.max_retries: 10
    livecomments.service.mysql.min_pool_size: 1
    livecomments.service.mysql.ro.connection_timeout_ms: 10000
    livecomments.service.mysql.ro.host: livecomments-mysql-ro
    livecomments.service.mysql.ro.idle_timeout_ms: 60000
    livecomments.service.mysql.ro.max_lifetime_ms: 600000
    livecomments.service.mysql.ro.max_pool_size: 5
    livecomments.service.mysql.ro.min_pool_size: 1
    livecomments.service.mysql.ro.user: livecomments
    livecomments.service.mysql.user: livecomments
    livecomments.service.request.timeout: 20000
    livecomments.service.server.port: 8080
    livecomments.service.settings_cache_max_size: 5000
    livecomments.service.settings_cache_ttl_m: 5
    livecomments.service.total_visible_content_cache_max_size: 2000
    livecomments.service.total_visible_content_cache_ttl_m: 5
    livecomments.service.trending_content_cache_max_size: 500
    livecomments.service.user_profile_cache_max_size: 200
    livecomments.service.user_profile_cache_ttl_m: 5
    moderation.kafka.bootstrap: "kafka:9092"
    publisher.kafka.bootstrap: "kafka:9092"
    section_tree.client.cache_expiry_sec: 15
    section_tree.client.service_url: "http://tyrion/v3/sections"
    section_tree.client.timeout_ms: 5000
    settings_client.cache_expiry_ms: 1
    settings_service.request_timeout_ms: 5000
    settings_service.url: "http://tyrion/v3/settings"
    user_actions.kafka.bootstrap: "kafka:9092"
    v2api.uri: "http://phpapi/v2"
    validation.originUrl.definedDomainCheck.enabled: true
  vfmetrics.properties:
    datadog.pattern: ".*"
    datadog.prefix: ''
    datadog.report-interval: 60
    datadog.tags: "environment:misconfigured"
    datadog.transport.type: udp
    heartbeat.enabled: true
    heartbeat.report-interval: 1
    namespace: livecomments
    reporters: ''