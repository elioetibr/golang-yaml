# yaml-language-server: $schema=values.schema.json
# Default values for base-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
# @schema
# enum: ["218894879100", "825534976873"]
# required: true
# additionalProperties: false

# @schema
# -- AWS Account ID
awsAccountId: "218894879100"

# @schema
# type: string
# additionalProperties: false
# @schema
# -- Override for chart name generation. When set, replaces the default chart name from Chart.yaml in resource naming
nameOverride: ""

# @schema
# type: string
# additionalProperties: false
# @schema
# -- Complete override for resource naming. When set, this exact name is used for all Kubernetes resources instead of generated names
fullnameOverride: ""

# @schema
# type: integer
# exclusiveMinimum: 0
# maximum: 500
# @schema
# -- Number of application pod replicas to maintain for high availability and load distribution
replicaCount: 1

# @schema
# additionalProperties: false
# @schema
# -- Kubernetes deployment strategy for managing pod updates and ensuring zero-downtime deployments
strategy:

  # -- Strategy type: RollingUpdate for gradual replacement, Recreate for immediate replacement
  type: RollingUpdate
  rollingUpdate:

    # -- Maximum extra pods allowed during update (can be number or percentage like "25%")
    maxSurge: 1

    # -- Maximum pods that can be unavailable during update to maintain service availability
    maxUnavailable: 0

# @schema
# additionalProperties: false
# @schema
# -- Container image configuration for the base-chart application
image:

  # -- ECR Registry Name
  name: livecomments-service
  # -- Docker image repository URL
  repository: 218894879100.dkr.ecr.us-east-1.amazonaws.com

  # @schema
  # additionalProperties: false
  # enum: [IfNotPresent,Always,Never]
  # default: Always
  # @schema
  # -- Image pull policy: IfNotPresent (cache locally), Always (always pull), Never (use local only)
  pullPolicy: Always
  # -- Specific image tag to deploy. When empty, uses the chart's appVersion from Chart.yaml
  tag: v5.12.1  # LATEST PRODUCTION VERSION

# @schema
# additionalProperties: true
# @schema
# -- List of secret names containing registry credentials for pulling images from private repositories like AWS ECR
# Example: [{name: "aws-ecr-secret"}]
imagePullSecrets: []

# @schema
# additionalProperties: false
# @schema
serviceAccount:

  # -- Create a dedicated ServiceAccount for this application (recommended for security)
  create: true
  # -- Automatically mount the ServiceAccount token for API access (required for most applications)
  automount: true
  # -- Custom annotations for the ServiceAccount (e.g., for AWS IAM role association)
  annotations: {}
  name: ""
  # -- AWS IAM role integration for pod-level permissions via IRSA (IAM Roles for Service Accounts)
  # Note: Choose either IRSA or Pod Identity, not both
  iamRole:

    # -- Enable IAM role association for accessing AWS services like S3, RDS, etc.
    enabled: true
    # -- ARN of the IAM role to associate with this ServiceAccount
    # Example: "arn:aws:iam::218894879100:role/base-chart-pod-role"
    roleArn: ""
    # -- Specific IAM role name to associate. If empty, uses generated format: {serviceAccount}.{namespace}.pod
    name: ""
  # -- AWS Pod Identity integration (newer alternative to IRSA)
  # Provides simplified IAM role association for pods without requiring OIDC configuration
  podIdentity:

    # -- Enable AWS Pod Identity for IAM role association
    enabled: false
    # -- ARN of the IAM role to associate with this ServiceAccount
    # Example: "arn:aws:iam::218894879100:role/base-chart-pod-role"
    roleArn: ""
    # -- Name of the Pod Identity Association. If empty, generates from release name
    associationName: ""
    # -- EKS cluster name for Pod Identity Association (required when Pod Identity is enabled)
    clusterName: ""
    # -- Use native AWS EKS Pod Identity Association instead of Crossplane CRD
    useNativeAssociation: false

# @schema
# type: object
# additionalProperties: false
# @schema
# -- RBAC configuration for pod permissions with the least privilege principle# Example cluster rules:
# Example custom rules:
# rbac:
#   role:
#     rules:
#       - apiGroups: [""]
#         resources: ["configmaps"]
#         verbs: ["get", "list"]
#   clusterRole:
#     rules:
#       - apiGroups: [""]
#         resources: ["nodes"]
#         verbs: ["get", "list"]
rbac:

  # -- Enable RBAC resource creation
  create: true
  # -- Role configuration for namespace-scoped permissions
  role:

    # -- Create a Role for namespace-scoped permissions
    create: true
    # -- Custom annotations for the Role
    annotations: {}
    rules: []
  clusterRole:

    # -- Create a ClusterRole for cluster-scoped permissions
    # Only enable if your application absolutely needs cluster-wide access
    create: false
    # -- Custom annotations for the ClusterRole
    annotations: {}
    rules: []

# @schema
# additionalProperties: true
# @schema
defaultAnnotations: {}

# @schema
# additionalProperties: true
# @schema
podAnnotations: {}

# @schema
# additionalProperties: true
# @schema
podLabels: {}

# @schema
# additionalProperties: true
# @schema
# -- Pod-level security context for controlling filesystem permissions and system settings
# Implements security best practices and least privilege principles
# Example sysctls for high-throughput applications:
# sysctls:
#   - name: net.ipv4.ip_local_port_range
#     value: "2048 64511"
podSecurityContext:

  # -- Run as non-root user for security
  runAsNonRoot: true
  # -- Specific user ID to run the container (1000 is typically a safe non-root user)
  runAsUser: 1000
  # -- Specific group ID for the container
  runAsGroup: 1000
  # -- Set filesystem group ownership for volumes
  fsGroup: 1000
  # -- Ensure filesystem group ownership changes are applied to volumes
  fsGroupChangePolicy: "OnRootMismatch"
  # -- Supplemental groups for the security context
  supplementalGroups: []
  seccompProfile:
    type: RuntimeDefault
  # -- Kernel parameters (sysctls) for pod-level tuning
  # Common for network stack optimization and connection handling
  sysctls: []

# @schema
# additionalProperties: false
# @schema
securityContext:

  # -- Drop all Linux capabilities for maximum security
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  # -- Prevent running as root user
  runAsNonRoot: true
  # -- Specific user ID to run the container
  runAsUser: 1000
  # -- Specific group ID for the container
  runAsGroup: 1000
  # -- Prevent privilege escalation
  allowPrivilegeEscalation: false
  # -- Set the seccomp profile to restrict system calls
  seccompProfile:
    type: RuntimeDefault

# @schema
# additionalProperties: false
# @schema
# -- CPU and memory resource management for predictable performance and cluster stability
# Resource allocation ensures:
# - Predictable performance through guaranteed resources (requests)
# - Protection against resource exhaustion through limits
# - Proper scheduling and bin-packing by Kubernetes scheduler
# Memory calculation for JVM applications:
# - Heap memory: Application objects and data
# - Non-heap: ~20-25% of heap (metaspace, code cache, etc.)
# - Direct memory: Off-heap buffers, typically matches heap size
# - Safety buffer: 20% overhead for OS and container
resources:
  limits:

    # -- Maximum CPU cores (1 core = 1000m). Throttles CPU usage to prevent noisy neighbor issues
    cpu: 400m
    # -- Maximum memory including heap, non-heap, and direct memory. Prevents OOM kills
    memory: 450Mi
  requests:

    # -- Guaranteed CPU allocation for consistent performance. Used by scheduler for placement
    cpu: 25m
    # -- Guaranteed memory allocation. Should match limits for memory-intensive applications
    memory: 450Mi

# @schema
# additionalProperties: true
# @schema
# -- Kubernetes Service configuration for network access and load balancing
service:

  # -- Short alias name used for service discovery and internal routing
  alias: base-chart
  # -- Full service name for DNS resolution within the cluster
  name: base-chart
  # -- Service Annotations
  annotations: {}
  portName: http
  # -- Network protocol: TCP for HTTP/WebSocket, UDP for streaming protocols
  protocol: TCP
  # -- Service type: ClusterIP (internal), LoadBalancer (external), NodePort (development)
  type: ClusterIP
  # -- External port exposed by the Service for client connections
  port: 80
  # -- Internal port the container listens on for incoming requests
  targetPort: 8080
  # -- NodePort for development/debugging (only when type is NodePort)
  nodePort:

    # -- Additional container ports configuration (JMX, metrics)
    containerPorts:

      # -- JMX (Java Management Extensions) port for JVM monitoring and management
      jmx:

        # -- Enable JMX port for Java application monitoring with tools like JConsole, VisualVM
        enabled: false
        # -- Expose JMX port through the main Service (false for security - use port-forwarding instead)
        exposeService: false
        # -- Named port identifier for JMX endpoint
        containerPortName: jmx
        # -- Port number for JMX connections (standard JMX port range)
        containerPort: 9016
        # -- Protocol for JMX communication (typically TCP)
        containerProtocol: TCP
      # -- Prometheus metrics port for application performance monitoring
      metrics:

        # -- Enable metrics endpoint for Prometheus scraping and monitoring dashboards
        enabled: true
        # -- Expose metrics port through Service for ServiceMonitor discovery
        exposeService: true
        # -- Named port identifier for metrics endpoint
        containerPortName: metrics
        # -- Port number for Prometheus metrics scraping (/metrics endpoint)
        containerPort: 5555
        # -- Protocol for metrics communication
        containerProtocol: TCP

# @schema
# additionalProperties: true
# @schema
# Health check configuration for automatic failure detection and recovery
# -- Liveness probe determines if the container is healthy and should be restarted if failing
# Used for detecting deadlocks, infinite loops, or unrecoverable application states
livenessProbe:
  enabled: true
  config:

    # -- HTTP health check endpoint that returns 200 OK when application is functioning
    httpGet:
      path: /healthy
      port: 8080
      scheme: HTTP
    # -- Wait time before starting health checks to allow application startup (JVM warmup, dependency connections)
    initialDelaySeconds: 40
    # -- Number of consecutive check failures before restarting the container (prevents flapping)
    failureThreshold: 5
    # -- Frequency of health checks during normal operation
    periodSeconds: 30
    # -- Number of consecutive successes to mark container as healthy again after failure
    successThreshold: 1
    # -- Maximum time to wait for health check response before marking as failed
    timeoutSeconds: 10

# @schema
# additionalProperties: true
# @schema
# -- Readiness probe determines if the container can accept traffic (added/removed from Service endpoints)
# Used for controlling traffic flow during startup, rolling updates, and temporary unavailability
readinessProbe:
  enabled: true
  config:

    # -- HTTP endpoint check to verify application is ready to serve requests
    httpGet:
      path: /healthy
      port: 8080
      scheme: HTTP
    # -- Delay before starting readiness checks (should be less than liveness to avoid restart loops)
    initialDelaySeconds: 60
    # -- Number of consecutive successes required to mark pod as ready for traffic
    successThreshold: 1
    # -- Number of consecutive failures before removing pod from Service load balancing
    failureThreshold: 5
    # -- Frequency of readiness checks (more frequent than liveness for responsive traffic management)
    periodSeconds: 10
    # -- Timeout for readiness check response
    timeoutSeconds: 5

# @schema
# additionalProperties: true
# @schema
# -- Startup probe protects slow-starting containers from being killed by liveness probe during initialization
# Gives applications extended time to complete startup procedures like database migrations, cache warming
startupProbe:
  enabled: true
  config:

    # -- HTTP endpoint to verify application has completed startup initialization
    httpGet:
      path: /healthy
      port: 8080
      scheme: HTTP
    # -- Delay before first startup check (minimal since this probe handles slow startups)
    initialDelaySeconds: 15
    # -- Maximum startup check failures allowed (total startup time = failureThreshold * periodSeconds)
    failureThreshold: 30
    # -- Frequency of startup checks during application initialization
    periodSeconds: 10
    # -- Single success marks startup complete, enabling liveness/readiness probes
    successThreshold: 1
    # -- Timeout for startup check response
    timeoutSeconds: 5

# @schema
# additionalProperties: true
# autoscaling:
# @schema
# -- Horizontal Pod Autoscaler for automatic scaling based on resource utilization and custom metrics
autoscaling:

  # -- Enable automatic pod scaling to handle varying traffic loads
  enabled: false
  # -- Minimum pod count to maintain for baseline capacity and availability
  minReplicas: 1
  # -- Maximum pod count to prevent resource exhaustion and cost overruns
  maxReplicas: 1
  # -- CPU utilization threshold
  targetCPUUtilizationPercentage: 33
  # targetMemoryUtilizationPercentage: 80  # Uncomment for memory-based scaling
  # @schema
  # additionalProperties: true
  # externalMetrics:
  # @schema
  # -- External metrics from Prometheus for custom scaling decisions
  externalMetrics:

    # -- Use external metrics like connection count, queue depth, or response time for scaling
    enabled: false
    # -- Prometheus metric name for active connection count monitoring
    connectionMetricName: base_connections_current
  # @schema
  # additionalProperties: true
  # @schema
  # -- Horizontal Pod Autoscaler Behavior
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        -
          type: Percent
          value: 10
          periodSeconds: 60
        -
          type: Pods
          value: 1
          periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        -
          type: Percent
          value: 100
          periodSeconds: 60
        -
          type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max

# @schema
# additionalProperties: true
# volumes:
# @schema
# -- Additional volumes for configuration files, secrets, or persistent storage
# Includes default temporary volumes for read-only filesystem compatibility
# Additional volume examples:
# - name: config-volume       # ConfigMap volume
#   configMap:
#     name: app-config
# - name: secret-volume       # Secret volume for certificates
#   secret:
#     secretName: tls-certs
#     optional: false
# - name: shared-storage      # Persistent volume
#   persistentVolumeClaim:
#     claimName: shared-data
volumes: []

# @schema
# additionalProperties: true
# volumeMounts:
# @schema
# -- Mount points for additional volumes inside the container filesystem
# Includes default mounts for read-only filesystem compatibility
# Default volume mounts for read-only root filesystem
# Additional mount examples:
# - name: config-volume
#   mountPath: "/app/config"       # Application configuration directory
#   readOnly: true
# - name: secret-volume
#   mountPath: "/etc/ssl/certs"    # SSL certificate location
#   readOnly: true
# - name: shared-storage
volumeMounts: []

# @schema
# arch:
#   type: string
#   enum: ["amd64", "arm64"]
# @schema
arch: amd64

# @schema
# additionalProperties: true
# @schema
# -- Pod Disruption Budget to maintain availability during cluster maintenance and updates
podDisruptionBudget:

  # -- Enable Pod Disruption Budget
  enabled: false
  # -- Maximum pods that can be unavailable during voluntary disruptions (node drains, updates)
  maxUnavailable: 1
# node-type: compute-optimized       # High-performance nodes
# zone: us-east-1a                   # Specific availability zone
nodeSelector: {}
tolerations: []
affinity: {}
podAffinity: []
topologySpreadConstraints: []
envVars:  # API_TIMEOUT: "30s"                  # External API timeout

  # CACHE_TTL: "300"                    # Cache time-to-live in seconds
  JAVA_OPTS: '-XX:NativeMemoryTracking=summary -Xms100m -Xmx100m -Dthumbnailator.conserveMemoryWorkaround=true -Dlog4j2.formatMsgNoLookups=True'
configMapEnvVars: {}
configMap:  # JWT_SECRET: signing-key                # Token signing secret

  # ENCRYPTION_KEY: data-encryption-key    # Data encryption key
  application.properties:
    auth.client_uuid: 0d0b1d94-4971-43ef-bdaa-8108ac1ffc55
    auth.refresh_token_backoff_multiplier: 1
    auth.refresh_token_interval_ms: 270000
    auth.refresh_token_max_attempts: 3
    auth.refresh_token_wait_duration_ms: 1000
    auth.service_uri: "https://auth/authorize_client"
    compliance.export.bucket: viafoura-gdpr-exports.misconfigured
    compliance.kafka.bootstrap: "kafka:9092"
    content_scores.kafka.bootstrap: "kafka:9092"
    customer.keys.get_retry_count: 30
    customer.keys.get_retry_wait_ms: 1000
    livecomments-trending.service.max_retries: 10
    livecomments-trending.service.mysql.db: livecomments
    livecomments-trending.service.mysql.ro.host: livecomments-mysql-trending
    livecomments-trending.service.mysql.ro.idle_timeout_ms: 60000
    livecomments-trending.service.mysql.ro.max_lifetime_ms: 600000
    livecomments-trending.service.mysql.ro.max_pool_size: 5
    livecomments-trending.service.mysql.ro.min_pool_size: 1
    livecomments-trending.service.mysql.ro.password: vf
    livecomments-trending.service.mysql.ro.user: livecomments
    livecomments.service.aws.bucket_name: origin-livecommentsmedia.env.net
    livecomments.service.cache.parent_site_cache_ttl_m: 10
    livecomments.service.cache.parent_site_max_cache_size: 1000
    livecomments.service.cache.relative_sections_cache_ttl_m: 5
    livecomments.service.cache.relative_sections_max_cache_size: 300
    livecomments.service.cache.site_group_cache_ttl_m: 5
    livecomments.service.cache.site_group_max_cache_size: 100
    livecomments.service.content_container_cache_max_size: 5000
    livecomments.service.content_container_cache_ttl_m: 5
    livecomments.service.content_container_comment_count_and_status_cache_max_size: 5000
    livecomments.service.content_container_comment_count_and_status_cache_ttl_m: 5
    livecomments.service.health-check.liveness.port: 8081
    livecomments.service.heartbeat_interval: 30000
    livecomments.service.max_content_page_size: 100
    livecomments.service.mysql.connection_timeout_ms: 10000
    livecomments.service.mysql.db: livecomments
    livecomments.service.mysql.host: livecomments-mysql
    livecomments.service.mysql.idle_timeout_ms: 60000
    livecomments.service.mysql.max_lifetime_ms: 60000
    livecomments.service.mysql.max_pool_size: 5
    livecomments.service.mysql.max_retries: 10
    livecomments.service.mysql.min_pool_size: 1
    livecomments.service.mysql.ro.connection_timeout_ms: 10000
    livecomments.service.mysql.ro.host: livecomments-mysql-ro
    livecomments.service.mysql.ro.idle_timeout_ms: 60000
    livecomments.service.mysql.ro.max_lifetime_ms: 600000
    livecomments.service.mysql.ro.max_pool_size: 5
    livecomments.service.mysql.ro.min_pool_size: 1
    livecomments.service.mysql.ro.user: livecomments
    livecomments.service.mysql.user: livecomments
    livecomments.service.request.timeout: 20000
    livecomments.service.server.port: 8080
    livecomments.service.settings_cache_max_size: 5000
    livecomments.service.settings_cache_ttl_m: 5
    livecomments.service.total_visible_content_cache_max_size: 2000
    livecomments.service.total_visible_content_cache_ttl_m: 5
    livecomments.service.trending_content_cache_max_size: 500
    livecomments.service.user_profile_cache_max_size: 200
    livecomments.service.user_profile_cache_ttl_m: 5
    moderation.kafka.bootstrap: "kafka:9092"
    publisher.kafka.bootstrap: "kafka:9092"
    section_tree.client.cache_expiry_sec: 15
    section_tree.client.service_url: "http://tyrion/v3/sections"
    section_tree.client.timeout_ms: 5000
    settings_client.cache_expiry_ms: 1
    settings_service.request_timeout_ms: 5000
    settings_service.url: "http://tyrion/v3/settings"
    user_actions.kafka.bootstrap: "kafka:9092"
    v2api.uri: "http://phpapi/v2"
    validation.originUrl.definedDomainCheck.enabled: true
  vfmetrics.properties:
    datadog.pattern: ".*"
    datadog.prefix: ''
    datadog.report-interval: 60
    datadog.tags: "environment:misconfigured"
    datadog.transport.type: udp
    heartbeat.enabled: true
    heartbeat.report-interval: 1
    namespace: livecomments
    reporters: ''
secretsEnvVars: {}
secrets:  # -- Argo Rollout configuration for advanced deployment strategies

  application.conf:  # SECRETS NAME USED BY THE APPLICATION

    auth.client_secret: viafoura
    livecomments.service.jwt.secret1: this is my secret
    livecomments.service.jwt.secret2: this is another secret
    livecomments.service.mysql.password: vf
    livecomments.service.mysql.ro.password: vf
rollout:
  # Alternative: minAvailable: 1  # Minimum pods that must remain available
  # @schema
  # additionalProperties: true
  # @schema
  # -- Node selector labels for scheduling pods on specific nodes with required characteristics
  # Use for dedicated nodes, specific hardware, or compliance requirements
  # Examples:
  # dedicated: base-chart
  # Dedicated node pool
  # @schema
  # additionalProperties: true
  # type: array
  # @schema
  # @schema
  # additionalProperties: true
  # @schema
  # -- Advanced pod scheduling rules for controlling pod placement relative to other pods and nodes
  # Provides fine-grained control over where pods are scheduled in the cluster for performance, compliance, and availability requirements.
  # Use cases:
  # - Co-location: Schedule pods near each other (e.g., app with cache)
  # - Anti-affinity: Spread pods across nodes/zones for high availability
  # - Node affinity: Target specific hardware, zones, or node types
  # - Mixed workload isolation: Separate different application tiers
  # Example configurations:
  # # Pod Anti-Affinity: Spread pods across different nodes for high availability
  # podAntiAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       podAffinityTerm:
  #         labelSelector:
  #           matchExpressions:
  #             - key: app.kubernetes.io/name
  #               operator: In
  #               values:
  #                 - base-chart
  #         topologyKey: kubernetes.io/hostname
  #
  # # Zone Anti-Affinity: Distribute pods across availability zones
  # podAntiAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 80
  #       podAffinityTerm:
  #         labelSelector:
  #           matchLabels:
  #             app.kubernetes.io/name: base-chart
  #         topologyKey: topology.kubernetes.io/zone
  #
  # # Node Affinity: Prefer compute-optimized nodes for CPU-intensive workloads
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       preference:
  #         matchExpressions:
  #           - key: node-type
  #             operator: In
  #             values:
  #               - compute-optimized
  #               - cpu-optimized
  #
  # # Required Node Affinity: Must run on nodes with SSD storage
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #       - matchExpressions:
  #           - key: storage-type
  #             operator: In
  #             values:
  #               - ssd
  #
  # # Pod Affinity: Co-locate with Redis cache for low latency
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       podAffinityTerm:
  #         labelSelector:
  #           matchLabels:
  # @schema
  # additionalProperties: true
  # @schema
  # -- Pod affinity configuration rules
  # - enabled: true
  #   weight: 100
  #   topologyKey: "kubernetes.io/hostname"
  #   excludeStable: false # Set to true if you want anti-affinity between canary and stable
  #   # For high traffic scenarios
  #   highTrafficMode:
  #     enabled: false
  #     weight: 50
  # - enabled: true
  #   weight: 80
  #   excludeStable: false # Set to true if you want anti-affinity between canary and stable
  #   topologyKey: "topology.kubernetes.io/zone"
  #   # For high traffic scenarios
  #   highTrafficMode:
  # @schema
  # additionalProperties: true
  # @schema
  # -- Topology spread constraints for pod distribution
  # - enabled: true
  #   maxSkew: 1
  #   topologyKey: "topology.kubernetes.io/zone"
  #   whenUnsatisfiable: "ScheduleAnyway"
  #   # For high traffic scenarios
  # @schema
  # additionalProperties: true
  # @schema
  # -- Environment variables for runtime configuration and JVM tuning
  # -- JVM configuration for memory management, garbage collection, and security
  # @schema
  # additionalProperties: true
  # @schema
  # -- Environment variables loaded from ConfigMaps for non-sensitive configuration
  # Useful for feature flags, API endpoints, logging levels, and operational settings
  # Example configuration:
  # LOG_LEVEL: INFO                     # Application logging verbosity
  # DEBUG_MODE: "false"                 # Enable debug features
  # @schema
  # additionalProperties: true
  # @schema
  # -- ConfigMap files for application configuration, properties, and settings
  # @schema
  # additionalProperties: true
  # @schema
  # -- Environment variables loaded from Secrets for sensitive configuration
  # Use for passwords, API keys, tokens, and other confidential data
  # Example sensitive configuration:
  # DATABASE_PASSWORD: postgres-password   # Database connection password
  # API_TOKEN: external-service-token      # Third-party API authentication
  # @schema
  # additionalProperties: true
  # @schema
  # -- Kubernetes Secret data for storing sensitive information securely
  # Values are automatically base64 encoded and encrypted at rest
  # Files are mounted to /var/migrator/jetty/resources via projected volume (optional, read-only)
  # Combined with configMap in a single mount point for unified configuration access
  # Example sensitive data:
  # database-password: "supersecret123"    # Database credentials
  # api-key: "sk-1234567890abcdef"         # External API key
  # @schema
  # additionalProperties: true
  # @schema
  # -- Whether to enable Argo Rollouts
  enabled: true
  # -- Number of old ReplicaSets to retain for rollback
  revisionHistoryLimit: 10

  # @schema
  # additionalProperties: true
  # @schema
  # -- Rollout strategy configuration
  strategy:


    # @schema
    # additionalProperties: true
    # blueGreen:
    # @schema
    # -- Blue-Green deployment strategy
    blueGreen:

      # -- Whether to enable blue-green deployments
      enabled: false
      # -- Whether to automatically promote new versions
      autoPromotionEnabled: false
      # -- Seconds to wait before auto-promotion
      autoPromotionSeconds: 30
      # -- Number of preview replicas (null uses main replica count)
      previewReplicaCount:

        # -- Seconds to wait before scaling down old version
        scaleDownDelaySeconds: 30
    # -- Canary deployment strategy
    # @schema
    # additionalProperties: true
    # canary:
    # @schema
    canary:

      # -- Whether to enable canary deployments
      enabled: true

      # @schema
      # additionalProperties: true
      # trafficRouting:
      # @schema
      # -- Istio traffic routing configuration for canary
      trafficRouting:


        # @schema
        # additionalProperties: true
        # istio:
        # @schema
        istio:


          # @schema
          # additionalProperties: true
          # virtualService:
          # @schema
          virtualService:
            name: ""  # Will be set via tpl in rollout template
            routes:
              - primary
          destinationRule:
            name: ""  # Will be set via tpl in rollout template
            canarySubsetName: canary
            stableSubsetName: stable
      # @schema
      # additionalProperties: true
      # managedRoutes:
      # @schema
      # -- Routes managed by the rollout controller
      managedRoutes: []

      # @schema
      # additionalProperties: true
      # steps:
      # @schema
      steps:
        -
          setWeight: 1
        -
          pause: {}
        -
          setWeight: 5
        -
          pause: {}
        -
          setWeight: 10
        -
          pause:
            duration: 10m
        -
          setWeight: 20
        -
          pause:
            duration: 10m
        -
          setWeight: 30
        -
          pause:
            duration: 10m
        -
          setWeight: 40
        -
          pause:
            duration: 10m
        -
          setWeight: 50
        -
          pause:
            duration: 10m
        -
          setWeight: 60
        -
          pause:
            duration: 10m
        -
          setWeight: 80
        -
          pause:
            duration: 10m
        -
          setWeight: 100
        -
          pause: {}
      maxUnavailable: 0
      # -- Maximum number of surge pods during canary
      maxSurge: 10%

  # @schema
  # additionalProperties: true
  # @schema
  # -- Analysis configuration for automated rollout decisions
  analysis:

    # -- Whether to enable rollout analysis
    enabled: false
    # -- Number of analysis runs to perform
    count: 3
    # -- Number of failures before rollback
    failureLimit: 1
    # -- Interval between analysis runs
    interval: 1m
    # -- Response time threshold in milliseconds
    responseTimeThreshold: 500
    # -- Step at which to start analysis
    startingStep: 2
    # -- Success condition threshold (0.99 = 99% success rate)
    successCondition: 0.99
    # -- Prometheus configuration for metrics collection
    prometheus:

      # -- Prometheus server address
      address: "http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090"
    # -- Analysis templates to use
    templates:      # -- Domain configuration for external access through Istio service mesh

      -
        templateName: success-rate

# @schema
# additionalProperties: false
# @schema
hosts:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Public domains accessible from the internet for client connections
  public:
    enabled: true

    # @schema
    # type: array
    # additionalProperties: true
    # @schema
    domains:      # -- Internal/Private Domains not accessible from the internet for client connections

      - livecomments.example.org
  # @schema
  # additionalProperties: true
  # @schema
  private:
    enabled: false

    # @schema
    # type: array
    # additionalProperties: true
    # @schema
    # -- Internal/Private Domains not accessible from the internet for client connections
    domains: []

# @schema
# additionalProperties: true
# @schema
istio:

  # -- Whether to enable Istio service mesh
  enabled: true
  # -- Global Istio configurations
  globals:

    # -- Global annotations for Istio resources
    annotations: {}
  ambient:

    # -- Whether to enable Ambient Mode (ztunnel mesh)
    enabled: true
    # -- Namespace labels for ambient mode enrollment
    namespaceLabels:
      istio.io/dataplane-mode: ambient
    # -- Waypoint proxy configuration for L7 features
    waypoint:

      # -- Whether to create a waypoint proxy for this service
      enabled: false
      # -- Traffic type for waypoint (service or workload)
      trafficType: service
  # -- Sidecar Mode configuration for envoy proxy injection
  sidecar:

    # -- Whether to enable Sidecar Mode (envoy proxy injection)
    enabled: false
    # -- Sidecar injection configuration
    injection:

      # -- Sidecar injection mode (auto, enabled, disabled)
      mode: auto
      # -- Pod-level sidecar injection annotation
      podAnnotation: sidecar.istio.io/inject
      # -- Namespace-level sidecar injection label
      namespaceLabel: istio-injection
    # -- Sidecar proxy configuration
    proxy:

      # -- Custom proxy configuration
      config: {}
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi
      # -- Sidecar proxy image (uses Istio default if empty)
      image: ""
      # -- Sidecar proxy log level
      logLevel: warning
  # -- Certificate Manager integration
  certManager:

    # -- Whether to enable cert-manager for TLS certificates
    enabled: false
    # -- Certificate issuer to use
    issuer: "letsencrypt-prod"
  # -- Istio Gateway configuration
  gateway:

    # -- Whether to create an Istio Gateway
    create: true
    # -- Gateway name (generated if empty)
    name: ""
    # -- Gateway annotations
    annotations: {}
    labels:
      external-dns: "true"
    # -- Gateway selector
    selector:
      istio: "ingressgateway"
    # -- Private Gateway selector to exposure service for private traffic
    private:
      selector:
        app: "istio-private-gateway"
    # -- Public Gateway selector to exposure service for public traffic
    public:
      selector:
        app: "istio-public-gateway"
    # -- Gateway server configurations
    servers:      # -- Additional gateway configuration options

      -
        # -- Whether this server configuration is enabled
        enabled: true
        # -- Port configurations
        ports:          # -- TLS configuration

          -
            number: 80
            name: http
            protocol: HTTP
        tls:

          # -- Whether TLS is enabled
          enabled: true
          # -- TLS configuration options
          config:

            # -- Whether to redirect HTTP to HTTPS
            httpsRedirect: true
      -
        # -- Whether this server configuration is enabled
        enabled: true
        # -- Port configurations for HTTPS
        ports:          # -- TLS configuration for HTTPS

          -
            number: 443
            name: https
            protocol: HTTP
        tls:

          # -- Whether TLS is enabled
          enabled: true
          # -- TLS configuration options
          config:

            # -- TLS mode (AUTO_PASSTHROUGH, SIMPLE, etc.)
            mode: AUTO_PASSTHROUGH
      -
        # -- Whether this server configuration is enabled
        enabled: false
        # -- Port configurations
        ports:          # -- TLS configuration

          -
            number: 80
            name: tcpsocket
            protocol: HTTP
        tls:

          # -- Whether TLS is enabled
          enabled: true
          # -- TLS configuration options
          config:

            # -- Whether to redirect HTTP to HTTPS
            httpsRedirect: true
    additionalConfig: {}
  virtualService:

    # -- Host for the VirtualService (defaults to service name)
    host: ""
    # -- Gateways to attach to the VirtualService
    gateways: []
    additionalRouteConfig: {}
    additionalHttp: []
  destinationRule:

    # -- Traffic policy for load balancing and connection pooling
    trafficPolicy:
      enabled: false
      config:
        connectionPool:
          tcp:
            maxConnections: 100
            connectTimeout: 30s
            tcpKeepalive:
              time: 7200s
              interval: 75s
          http:
            http1MaxPendingRequests: 64
            http2MaxRequests: 1000
            maxRequestsPerConnection: 10
            maxRetries: 3
        loadBalancer:
          simple: LEAST_CONN
        outlierDetection:
          consecutiveGatewayErrors: 5
          consecutive5xxErrors: 5
          interval: 30s
          baseEjectionTime: 30s
          maxEjectionPercent: 50
  # -- Fault injection configuration for testing
  faultInjection:

    # -- Whether to enable fault injection
    enabled: false
    # -- Fault injection configurations
    configs: {}

# @schema
# additionalProperties: true
# @schema
serviceMonitor:

  # -- Whether to enable ServiceMonitor for Prometheus scraping
  enabled: true
  # -- Namespace where ServiceMonitor should be deployed (if different from app namespace)
  namespace: ""
  # -- Prometheus release name for ServiceMonitor
  prometheusReleaseName: kube-prometheus-stack
  # -- Scraping interval for metrics
  interval: 10s
  # -- Scraping timeout for metrics (should be less than interval)
  scrapeTimeout: 5s
  # -- Metrics endpoint path
  path: /metrics

  # @schema
  # additionalProperties: true
  # labels:
  # @schema
  # -- Additional labels for ServiceMonitor
  labels: {}
  namespaceSelector:

    # -- Enable any namespace selector (allows monitoring across namespaces)
    any: false
    # -- Specific namespaces to monitor (leave empty for current namespace only)
    matchNames: []

# @schema
# additionalProperties: false
# @schema
# -- Datadog configuration
# Alternative configuration for collecting all metrics
# additionalProperties: true
# datadog:
#   enabled: true
#   containerName: "base-chart"
#   namespace: "base-chart"
#   metricsPath: "/metrics"
#   collectAllMetrics: true
#   ignoreMetrics:
#     - "go_.*"
#     - "process_.*"
#     - "prometheus_.*"
#   tags:
datadog:

  # -- Enable/disable Datadog monitoring
  enabled: true
  # -- Container name (should match your main container name)
  containerName: "base-chart"
  # -- Namespace prefix for metrics in Datadog
  namespace: "base-chart"
  # -- Metrics endpoint path
  metricsPath: "/metrics"
  # -- Collect all metrics (if true, ignores specific metrics list)
  collectAllMetrics: true
  # -- Timeout for metric collection
  timeout: 20
  # -- Enable health service check
  healthServiceCheck: true
  # -- Send histogram buckets
  sendHistogramsBuckets: true
  # -- Collect histogram buckets
  collectHistogramBuckets: true
  # -- Specific metrics to collect with transformations
  # Can be either strings (collect as-is) or objects (rename)
  metrics:    # -- Metrics to ignore/exclude

    -
      # Collect these metrics as-is
      "http_requests_total"
    - "http_request_duration_seconds"
    - "database_connections_active"
    -
      source: "go_memstats_alloc_bytes"
      target: "memory_allocated"
    -
      source: "process_cpu_seconds_total"
      target: "cpu_usage_seconds"
    -
      source: "base_custom_metric_total"
      target: "custom_operations"
    -
      source: "prometheus_rule_evaluation_duration_seconds"
      target: "rule_evaluation_time"
  ignoreMetrics:
    - "go_gc_.*"
    - "go_goroutines"
    - "process_.*_bytes"
    - "prometheus_.*"
    - "up"
  metricTransformations:
    labels:      # -- Additional tags to add to all metrics

      # Note: version tag will be automatically replaced with the actual app version from Chart.yaml or image.tag
      -
        from: "handler"
        to: "endpoint"
      -
        from: "method"
        to: "http_method"
      -
        from: "status_code"
        to: "response_code"
  tags:
    env: production
    team: backend
    service: base-chart
  logs:
    enabled: true
    source: "base-chart"
    service: "base-chart"
    # Log processing rules
    logProcessingRules:      # -- JMX (Java Management Extensions) integration configuration

      # Complete configuration for JMX monitoring with Datadog Agent
      -
        type: "exclude_at_match"
        name: "exclude_health_checks"
        pattern: "GET /health"
      -
        type: "mask_sequences"
        name: "mask_credit_cards"
        pattern: "\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}"
        replacement: "****-****-****-****"
  jmx:

    # -- JMX connection URL (auto-generated if not provided)
    # Example: "service:jmx:rmi:///jndi/rmi://%%host%%:9016/jmxrmi"
    jmxUrl: ""
    # -- JMX authentication username
    user: ""
    # -- JMX authentication password (store in secret for production)
    password: ""
    # -- Whether this is a JMX integration (default: true when jmx integration is selected)
    isJmx: true
    # -- Collect default JMX metrics (heap, threads, GC, etc.)
    collectDefaultMetrics: true
    # -- Process name regex for JMX connection discovery
    # Example: ".*java.*base-chart.*"
    processNameRegex: ""
    # -- Custom tools.jar path for JMX tools
    tools_jar_path: ""
    # -- Custom name identifier for this JMX instance
    name: ""
    # -- Custom Java binary path
    java_bin_path: ""
    # -- JVM options for JMX connection
    # Example: "-Xms64m -Xmx128m"
    java_options: ""
    # -- SSL/TLS trust store path for secure JMX connections
    trust_store_path: ""
    # -- SSL/TLS trust store password
    trust_store_password: ""
    # -- SSL/TLS key store path for client certificates
    key_store_path: ""
    # -- SSL/TLS key store password
    key_store_password: ""
    # -- Enable SSL for RMI registry connections
    rmi_registry_ssl: false
    # -- RMI connection timeout in milliseconds
    rmi_connection_timeout: 20000
    # -- RMI client timeout in milliseconds
    rmi_client_timeout: 15000
    # -- Collect default JVM metrics (memory, GC, threads)
    collect_default_jvm_metrics: true
    # -- Enable new garbage collection metrics format
    new_gc_metrics: true
    # -- Custom service check prefix for JMX health checks
    service_check_prefix: "jmx"
    # -- Custom metric configuration for JMX beans
    # Define specific MBeans and attributes to collect
    conf:      # -- Custom integration configuration

      # For integrations other than openmetrics or jmx
      -
        # Example JMX metric collection configuration
        include:
          domain: "java.lang"
          type: "Memory"
          attribute:
            HeapMemoryUsage:
              alias: "jvm.heap_memory"
              metric_type: "gauge"
            NonHeapMemoryUsage:
              alias: "jvm.non_heap_memory"
              metric_type: "gauge"
      -
        include:
          domain: "java.lang"
          type: "GarbageCollector"
          name: "*"
          attribute:
            CollectionCount:
              alias: "jvm.gc.cms.count"
              metric_type: "counter"
            CollectionTime:
              alias: "jvm.gc.cms.time"
              metric_type: "counter"
      -
        include:
          domain: "java.lang"
          type: "Threading"
          attribute:
            ThreadCount:
              alias: "jvm.thread_count"
              metric_type: "gauge"
            DaemonThreadCount:
              alias: "jvm.daemon_thread_count"
              metric_type: "gauge"
      -
        # Application-specific MBeans (customize for your application)
        include:
          domain: "com.your.app"
          type: "ConnectionPool"
          attribute:
            ActiveConnections:
              alias: "app.db.active_connections"
              metric_type: "gauge"
            IdleConnections:
              alias: "app.db.idle_connections"
              metric_type: "gauge"
  instanceConfig: {}
  # Example custom integration:
  # host: "%%host%%"
  # port: 8080
  # username: "monitor"
  # password: "secret"
  # ssl: true
  # timeout: 30
  # -- Integration type: openmetrics, jmx, or custom
  # openmetrics: For Prometheus-style metrics endpoints
  integration: "jmx"
  # -- Custom init configuration for the integration
  # Additional configuration passed to the integration's init_config
  initConfig: {}
  apm:
    enabled: false
    serviceName: "base-chart"
    environment: "production"

# @schema
# additionalProperties: true
# @schema
# -- NetworkPolicy configuration for defense-in-depth network security
# Implements zero-trust networking with explicit allow rules for required communication
networkPolicy:

  # -- Enable NetworkPolicy creation for enhanced network security
  enabled: false
  # -- Deny-all baseline policy configuration
  denyAll:

    # -- Create a deny-all NetworkPolicy as security baseline (recommended for production)
    enabled: false
  # -- Ingress traffic rules configuration
  ingress:

    # -- Allow ingress from pods within the same namespace
    allowSameNamespace: true
    # -- Allow ingress from Istio Gateway for service mesh traffic
    allowIstioGateway: true
    # -- Allow ingress from monitoring namespace for metrics collection
    allowMonitoring: true
    # -- Name of the monitoring namespace (default: monitoring)
    monitoringNamespace: "monitoring"
    # -- Custom ingress rules for specific traffic requirements
    customRules: []
  egress:
    # Example custom rules:
    # - namespaceSelector:
    #     matchLabels:
    #       name: "api-gateway"
    #   ports:
    #     - protocol: TCP
    #       port: 8080
    # - ipBlock:
    #     cidr: "10.0.0.0/8"
    #   ports:
    #     - protocol: TCP
    #       port: 8080
    # -- Enable egress policies (when false, all egress is allowed)
    enabled: false
    # -- Allow DNS resolution (required for service discovery)
    allowDNS: true
    # -- Allow HTTPS traffic to internet (for external APIs, image pulls)
    allowHTTPS: true
    # -- Allow traffic within the same namespace
    allowSameNamespace: true
    # -- Allow access to AWS metadata service (required for AWS integrations)
    allowAWSMetadata: true
    # -- Custom egress rules for specific external services
    customRules: []
  monitoring:
    # Example custom rules:
    # - namespaceSelector:
    #     matchLabels:
    #       name: "database"
    #   ports:
    #     - protocol: TCP
    #       port: 5432
    # - ipBlock:
    #     cidr: "192.168.1.0/24"
    #   ports:
    #     - protocol: TCP
    #       port: 443
    # -- Create dedicated NetworkPolicy for monitoring access
    enabled: true
    # -- Prometheus namespace for metrics scraping
    prometheusNamespace: "monitoring"
    # -- Datadog namespace for agent access
    datadogNamespace: "datadog"

# @schema
# additionalProperties: true
# @schema
# -- Pod Security Standards configuration for compliance automation
# Implements Kubernetes Pod Security Standards for enhanced security posture
podSecurityStandards:

  # -- Enable Pod Security Standards compliance features
  enabled: true
  # -- Pod Security Standards level: privileged, baseline, or restricted
  # restricted: Most secure, suitable for security-critical applications
  # baseline: Minimally restrictive, prevents known privilege escalations
  # privileged: Unrestricted, suitable for system-level workloads
  level: "restricted"
  # -- Create PodSecurityPolicy for clusters without Pod Security Standards
  # (Kubernetes < 1.25 or clusters still using PSP)
  createPodSecurityPolicy: false
  # -- Create SecurityContextConstraints for OpenShift clusters
  createSecurityContextConstraints: false
  # -- Allow hostPath volumes (not recommended for restricted level)
  allowHostPaths: false
  # -- Allowed hostPath configurations (only if allowHostPaths is true)
  allowedHostPaths: []
  # Example:
  # - pathPrefix: "/tmp"
  #   readOnly: true
  # - pathPrefix: "/var/log"
  allowedHostPortRanges: []
  # Example:
  # - min: 8000
  allowedCapabilities: []
  # Example (not recommended for restricted):
  # - "NET_BIND_SERVICE"
  allowedSeccompProfiles: []
  # Example:
  # - type: "Localhost"
  apparmor:

    # -- Enable AppArmor profiles
    enabled: false
    # -- AppArmor profiles for containers
    profiles: {}
  compliance:
    # Example:
    # base-chart: "runtime/default"
    # -- Enable compliance monitoring and reporting
    enabled: true
    # -- Compliance frameworks to validate against
    frameworks:
      - "pod-security-standards"
      - "cis-kubernetes"
      - "nist-cybersecurity"
    auditLogging:
      enabled: true
      # -- Violation severity levels to log
      levels:        # -- Backup automation configuration for operational excellence

        # Provides multiple backup strategies including Velero and custom CronJob backups
        - "warn"
        - "error"
# @schema
# additionalProperties: true
# @schema
backup:

  # -- Enable backup automation features
  enabled: false
  # -- Velero backup configuration for full cluster backup
  velero:

    # -- Enable Velero backup scheduling
    enabled: false
    # -- Backup schedule in cron format (default: daily at 2 AM)
    schedule: "0 2 * * *"
    # -- Backup retention period (default: 30 days)
    retention: "720h"
    # -- Velero namespace (default: velero)
    namespace: "velero"
    # -- Storage location for backups
    storageLocation: "default"
    # -- Volume snapshot locations for persistent volumes
    volumeSnapshotLocations:
      - "default"
    includeCustomResources: true
    # -- Custom resources to include in backup
    customResources:
      - "rollouts.argoproj.io"
      - "virtualservices.networking.istio.io"
      - "destinationrules.networking.istio.io"
      - "gateways.networking.istio.io"
    hooks:
      enabled: true
  # -- Custom CronJob backup for application-specific data
  cronjob:

    # -- Enable CronJob-based backup
    enabled: false
    # -- Backup schedule in cron format (default: daily at 3 AM)
    schedule: "0 3 * * *"
    # -- Timezone for backup schedule
    timeZone: "UTC"
    # -- Concurrency policy for backup jobs
    concurrencyPolicy: "Forbid"
    # -- Number of successful backup jobs to retain
    successfulJobsHistoryLimit: 3
    # -- Number of failed backup jobs to retain
    failedJobsHistoryLimit: 1
    # -- Deadline for backup job to start
    startingDeadlineSeconds: 300
    # -- Number of retries for failed backup jobs
    backoffLimit: 2
    # -- Maximum time for backup job to complete
    activeDeadlineSeconds: 3600
    # -- Retention period for backups (in days)
    retentionDays: 30
    # -- Type of backup to perform
    backupType: "application-data"
    # -- Container image for backup executor
    image:
      repository: "amazon/aws-cli"
      tag: "2.13.0"
      pullPolicy: "IfNotPresent"
    # -- Resource limits and requests for backup job
    resources:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "128Mi"
    # -- S3 configuration for backup storage
    s3:

      # -- S3 bucket for backup storage
      bucket: ""
      # -- S3 prefix for backup objects
      prefix: "backups"
    # -- Additional environment variables for backup job
    extraEnv: {}
    # Example:
    # AWS_REGION: "us-east-1"
    volumes: []
    # Example:
    # - name: config
    #   configMap:
    volumeMounts: []
    # Example:
    # - name: config
    #   mountPath: /etc/backup
    nodeSelector: {}
    # Example:
    # kubernetes.io/arch: amd64
    tolerations: []
  monitoring:
    # Example:
    # - key: "backup-node"
    #   operator: "Equal"
    #   value: "true"
    # -- Enable backup monitoring and metrics
    enabled: true
    # -- Prometheus metrics for backup status
    metrics:

      # -- Enable backup metrics collection
      enabled: true
      # -- Metrics endpoint path
      path: "/metrics"
      # -- Metrics collection interval
      interval: "30s"
    # -- Alerting configuration for backup failures
    alerting:

      # -- Enable backup failure alerts
      enabled: true
      # -- Alert severity levels
      severity: "warning"
      # -- Alert notification channels
      channels:        # -- Grafana Dashboards Configuration

        - "slack"
        - "email"
# @schema
# additionalProperties: true
# @schema
grafana:

  # -- Dashboard configuration
  dashboards:

    # -- Enable Grafana dashboard ConfigMap creation
    # Only creates ConfigMap if dashboard file exists at dashboards/base-chart.json
    enabled: false
    # -- Grafana folder for dashboard organization
    folder: "Base-Chart"
    # -- Additional labels for dashboard ConfigMap discovery
    labels:
      grafanaDashboard: "1"
      dashboardSource: "base-chart"
    # @schema
    # additionalProperties: true
    # type: object
    # @schema
    # -- Grafana Dashboard Annotations
    # Example annotations:
    # grafana.com/dashboard-uid: "base-chart-dashboard"
    # grafana.com/dashboard-title: "Base-Chart Service Monitoring"
    annotations: {}