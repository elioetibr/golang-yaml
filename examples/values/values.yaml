# yaml-language-server: $schema=values.schema.json
# Default values for base-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# @schema
# enum: ["218894879100", "825534976873"]
# required: true
# additionalProperties: false
# @schema
# -- AWS Account ID
awsAccountId: "218894879100"

# @schema
# type: string
# additionalProperties: false
# @schema
# -- Override for chart name generation. When set, replaces the default chart name from Chart.yaml in resource naming
nameOverride: ""

# @schema
# type: string
# additionalProperties: false
# @schema
# -- Complete override for resource naming. When set, this exact name is used for all Kubernetes resources instead of generated names
fullnameOverride: ""

# @schema
# type: integer
# exclusiveMinimum: 0
# maximum: 500
# @schema
# -- Number of application pod replicas to maintain for high availability and load distribution
replicaCount: 1

# @schema
# additionalProperties: false
# @schema
# -- Kubernetes deployment strategy for managing pod updates and ensuring zero-downtime deployments
strategy:

  # -- Strategy type: RollingUpdate for gradual replacement, Recreate for immediate replacement
  type: RollingUpdate
  rollingUpdate:

    # -- Maximum extra pods allowed during update (can be number or percentage like "25%")
    maxSurge:

      # -- Maximum pods that can be unavailable during update to maintain service availability
      1
    maxUnavailable:


      # @schema
      # additionalProperties: false
      # @schema
      # -- Container image configuration for the base-chart application
      0
image:

  # -- ECR Registry Name
  name: livecomments-service
  # -- Docker image repository URL
  repository: 218894879100.dkr.ecr.us-east-1.amazonaws.com

  # @schema
  # additionalProperties: false
  # enum: [IfNotPresent,Always,Never]
  # default: Always
  # @schema
  # -- Image pull policy: IfNotPresent (cache locally), Always (always pull), Never (use local only)
  pullPolicy: Always
  # -- Specific image tag to deploy. When empty, uses the chart's appVersion from Chart.yaml
  tag: v5.12.1
imagePullSecrets:


  # @schema
  # additionalProperties: false
  # @schema
  # -- ServiceAccount configuration for pod identity and AWS IAM integration
[]
serviceAccount:

  # -- Create a dedicated ServiceAccount for this application (recommended for security)
  create:

    # -- Automatically mount the ServiceAccount token for API access (required for most applications)
    true
  automount:

    # -- Custom annotations for the ServiceAccount (e.g., for AWS IAM role association)
    true
  annotations:

    # -- Custom name for the ServiceAccount. If empty, generates from the release name
{}
  name:

    # -- AWS IAM role integration for pod-level permissions via IRSA (IAM Roles for Service Accounts)
    # Note: Choose either IRSA or Pod Identity, not both
    ""
  iamRole:

    # -- Enable IAM role association for accessing AWS services like S3, RDS, etc.
    enabled:

      # -- ARN of the IAM role to associate with this ServiceAccount
      # Example: "arn:aws:iam::218894879100:role/base-chart-pod-role"
      true
    roleArn:

      # -- Specific IAM role name to associate. If empty, uses generated format: {serviceAccount}.{namespace}.pod
      ""
    name:

      # -- AWS Pod Identity integration (newer alternative to IRSA)
      # Provides simplified IAM role association for pods without requiring OIDC configuration
      ""
  podIdentity:

    # -- Enable AWS Pod Identity for IAM role association
    enabled:

      # -- ARN of the IAM role to associate with this ServiceAccount
      # Example: "arn:aws:iam::218894879100:role/base-chart-pod-role"
      false
    roleArn:

      # -- Name of the Pod Identity Association. If empty, generates from release name
      ""
    associationName:

      # -- EKS cluster name for Pod Identity Association (required when Pod Identity is enabled)
      ""
    clusterName:

      # -- Use native AWS EKS Pod Identity Association instead of Crossplane CRD
      ""
    useNativeAssociation:


      # @schema
      # type: object
      # additionalProperties: false
      # @schema
      # -- RBAC configuration for pod permissions with the least privilege principle# Example cluster rules:
      # Example custom rules:
      # rbac:
      #   role:
      #     rules:
      #       - apiGroups: [""]
      #         resources: ["configmaps"]
      #         verbs: ["get", "list"]
      #   clusterRole:
      #     rules:
      #       - apiGroups: [""]
      #         resources: ["nodes"]
      #         verbs: ["get", "list"]
      false
rbac:

  # -- Enable RBAC resource creation
  create:

    # -- Role configuration for namespace-scoped permissions
    true
  role:

    # -- Create a Role for namespace-scoped permissions
    create:

      # -- Custom annotations for the Role
      true
    annotations:

      # -- Custom rules for the Role (will be merged with default minimal rules)
      # Use this to add additional permissions your application needs
{}
    rules:

      # -- ClusterRole configuration for cluster-scoped permissions (use sparingly)
[]
  clusterRole:

    # -- Create a ClusterRole for cluster-scoped permissions
    # Only enable if your application absolutely needs cluster-wide access
    create:

      # -- Custom annotations for the ClusterRole
      false
    annotations:

      # -- Custom rules for the ClusterRole
      # Only add rules that require cluster-wide access
{}
    rules:


      # @schema
      # additionalProperties: true
      # @schema
      # -- Global annotations applied to all Kubernetes resources created by this chart
      # Example: {"app.kubernetes.io/owner": "platform-team"}
[]
defaultAnnotations:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Custom annotations for pods, useful for monitoring, networking, and security policies
  # Example: {"prometheus.io/scrape": "true", "linkerd.io/inject": "enabled"}
{}
podAnnotations:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Custom labels for pods used for selection, monitoring, and organization
  # Example: {"team": "backend", "component": "event-processor"}
{}
podLabels:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Pod-level security context for controlling filesystem permissions and system settings
  # Implements security best practices and least privilege principles
  # Example sysctls for high-throughput applications:
  # sysctls:
  #   - name: net.ipv4.ip_local_port_range
  #     value: "2048 64511"
  #   - name: net.core.somaxconn
  #     value: "16384"
{}
podSecurityContext:

  # -- Run as non-root user for security
  runAsNonRoot:

    # -- Specific user ID to run the container (1000 is typically a safe non-root user)
    true
  runAsUser:

    # -- Specific group ID for the container
    1000
  runAsGroup:

    # -- Set filesystem group ownership for volumes
    1000
  fsGroup:

    # -- Ensure filesystem group ownership changes are applied to volumes
    1000
  fsGroupChangePolicy:

    # -- Supplemental groups for the security context
    "OnRootMismatch"
  supplementalGroups:

    # -- Set the seccomp profile to restrict system calls
[]
  seccompProfile:
    type:

      # -- Kernel parameters (sysctls) for pod-level tuning
      # Common for network stack optimization and connection handling
      RuntimeDefault
  sysctls:


    # @schema
    # additionalProperties: false
    # @schema
    # -- Container-level security context for privilege control and attack surface reduction
    # Implements defense-in-depth security principles
[]
securityContext:

  # -- Drop all Linux capabilities for maximum security
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem:

    # -- Prevent running as root user
    true
  runAsNonRoot:

    # -- Specific user ID to run the container
    true
  runAsUser:

    # -- Specific group ID for the container
    1000
  runAsGroup:

    # -- Prevent privilege escalation
    1000
  allowPrivilegeEscalation:

    # -- Set the seccomp profile to restrict system calls
    false
  seccompProfile:
    type:


      # @schema
      # additionalProperties: false
      # @schema
      # -- CPU and memory resource management for predictable performance and cluster stability
      # Resource allocation ensures:
      # - Predictable performance through guaranteed resources (requests)
      # - Protection against resource exhaustion through limits
      # - Proper scheduling and bin-packing by Kubernetes scheduler
      # Memory calculation for JVM applications:
      # - Heap memory: Application objects and data
      # - Non-heap: ~20-25% of heap (metaspace, code cache, etc.)
      # - Direct memory: Off-heap buffers, typically matches heap size
      # - Safety buffer: 20% overhead for OS and container
      RuntimeDefault
resources:
  limits:

    # -- Maximum CPU cores (1 core = 1000m). Throttles CPU usage to prevent noisy neighbor issues
    cpu: 400m
    # -- Maximum memory including heap, non-heap, and direct memory. Prevents OOM kills
    memory: 450Mi
  requests:

    # -- Guaranteed CPU allocation for consistent performance. Used by scheduler for placement
    cpu: 25m
    # -- Guaranteed memory allocation. Should match limits for memory-intensive applications
    memory: 450Mi
service:

  # -- Short alias name used for service discovery and internal routing
  alias: base-chart
  # -- Full service name for DNS resolution within the cluster
  name: base-chart
  # -- Service Annotations
  annotations:

    # -- Named port for the main HTTP service endpoint
{}
  portName: http
  # -- Network protocol: TCP for HTTP/WebSocket, UDP for streaming protocols
  protocol: TCP
  # -- Service type: ClusterIP (internal), LoadBalancer (external), NodePort (development)
  type: ClusterIP
  # -- External port exposed by the Service for client connections
  port: 80
  # -- Internal port the container listens on for incoming requests
  targetPort: 8080
  # -- NodePort for development/debugging (only when type is NodePort)
  nodePort:

    # -- Additional container ports configuration (JMX, metrics)
    containerPorts:

      # -- JMX (Java Management Extensions) port for JVM monitoring and management
      jmx:

        # -- Enable JMX port for Java application monitoring with tools like JConsole, VisualVM
        enabled:

          # -- Expose JMX port through the main Service (false for security - use port-forwarding instead)
          false
        exposeService:

          # -- Named port identifier for JMX endpoint
          false
        containerPortName:

          # -- Port number for JMX connections (standard JMX port range)
          jmx
        containerPort:

          # -- Protocol for JMX communication (typically TCP)
          9016
        containerProtocol:

          # -- Prometheus metrics port for application performance monitoring
          TCP
      metrics:

        # -- Enable metrics endpoint for Prometheus scraping and monitoring dashboards
        enabled:

          # -- Expose metrics port through Service for ServiceMonitor discovery
          true
        exposeService:

          # -- Named port identifier for metrics endpoint
          true
        containerPortName:

          # -- Port number for Prometheus metrics scraping (/metrics endpoint)
          metrics
        containerPort:

          # -- Protocol for metrics communication
          5555
        containerProtocol:


          # @schema
          # additionalProperties: true
          # @schema
          # Health check configuration for automatic failure detection and recovery
          # -- Liveness probe determines if the container is healthy and should be restarted if failing
          # Used for detecting deadlocks, infinite loops, or unrecoverable application states
          TCP
livenessProbe:
  enabled: true
  config:

    # -- HTTP health check endpoint that returns 200 OK when application is functioning
    httpGet:
      path: /healthy
      port: 8080
      scheme:

        # -- Wait time before starting health checks to allow application startup (JVM warmup, dependency connections)
        HTTP
    initialDelaySeconds:

      # -- Number of consecutive check failures before restarting the container (prevents flapping)
      40
    failureThreshold:

      # -- Frequency of health checks during normal operation
      5
    periodSeconds:

      # -- Number of consecutive successes to mark container as healthy again after failure
      30
    successThreshold:

      # -- Maximum time to wait for health check response before marking as failed
      1
    timeoutSeconds:


      # @schema
      # additionalProperties: true
      # @schema
      # -- Readiness probe determines if the container can accept traffic (added/removed from Service endpoints)
      # Used for controlling traffic flow during startup, rolling updates, and temporary unavailability
      10
readinessProbe:
  enabled: true
  config:

    # -- HTTP endpoint check to verify application is ready to serve requests
    httpGet:
      path: /healthy
      port: 8080
      scheme:

        # -- Delay before starting readiness checks (should be less than liveness to avoid restart loops)
        HTTP
    initialDelaySeconds:

      # -- Number of consecutive successes required to mark pod as ready for traffic
      60
    successThreshold:

      # -- Number of consecutive failures before removing pod from Service load balancing
      1
    failureThreshold:

      # -- Frequency of readiness checks (more frequent than liveness for responsive traffic management)
      5
    periodSeconds:

      # -- Timeout for readiness check response
      10
    timeoutSeconds:


      # @schema
      # additionalProperties: true
      # @schema
      # -- Startup probe protects slow-starting containers from being killed by liveness probe during initialization
      # Gives applications extended time to complete startup procedures like database migrations, cache warming
      5
startupProbe:
  enabled: true
  config:

    # -- HTTP endpoint to verify application has completed startup initialization
    httpGet:
      path: /healthy
      port: 8080
      scheme:

        # -- Delay before first startup check (minimal since this probe handles slow startups)
        HTTP
    initialDelaySeconds:

      # -- Maximum startup check failures allowed (total startup time = failureThreshold * periodSeconds)
      15
    failureThreshold:

      # -- Frequency of startup checks during application initialization
      30
    periodSeconds:

      # -- Single success marks startup complete, enabling liveness/readiness probes
      10
    successThreshold:

      # -- Timeout for startup check response
      1
    timeoutSeconds:


      # @schema
      # additionalProperties: true
      # autoscaling:
      # @schema
      # -- Horizontal Pod Autoscaler for automatic scaling based on resource utilization and custom metrics
      5
autoscaling:

  # -- Enable automatic pod scaling to handle varying traffic loads
  enabled: false
  # -- Minimum pod count to maintain for baseline capacity and availability
  minReplicas: 1
  # -- Maximum pod count to prevent resource exhaustion and cost overruns
  maxReplicas: 1
  # -- CPU utilization threshold
  targetCPUUtilizationPercentage: 33
  # targetMemoryUtilizationPercentage: 80  # Uncomment for memory-based scaling
  # @schema
  # additionalProperties: true
  # externalMetrics:
  # @schema
  # -- External metrics from Prometheus for custom scaling decisions
  externalMetrics:

    # -- Use external metrics like connection count, queue depth, or response time for scaling
    enabled:

      # -- Prometheus metric name for active connection count monitoring
      false
    connectionMetricName:


      # @schema
      # additionalProperties: true
      # @schema
      # -- Horizontal Pod Autoscaler Behavior
      base_connections_current
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        -
          type: Percent
          value: 10
          periodSeconds: 60
        -
          type: Pods
          value: 1
          periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        -
          type: Percent
          value: 100
          periodSeconds: 60
        -
          type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy:


        # @schema
        # additionalProperties: true
        # volumes:
        # @schema
        # -- Additional volumes for configuration files, secrets, or persistent storage
        # Includes default temporary volumes for read-only filesystem compatibility
        # Additional volume examples:
        # - name: config-volume       # ConfigMap volume
        #   configMap:
        #     name: app-config
        # - name: secret-volume       # Secret volume for certificates
        #   secret:
        #     secretName: tls-certs
        #     optional: false
        # - name: shared-storage      # Persistent volume
        #   persistentVolumeClaim:
        #     claimName: shared-data
        Max
volumes:


  # @schema
  # additionalProperties: true
  # volumeMounts:
  # @schema
  # -- Mount points for additional volumes inside the container filesystem
  # Includes default mounts for read-only filesystem compatibility
  # Default volume mounts for read-only root filesystem
  # Additional mount examples:
  # - name: config-volume
  #   mountPath: "/app/config"       # Application configuration directory
  #   readOnly: true
  # - name: secret-volume
  #   mountPath: "/etc/ssl/certs"    # SSL certificate location
  #   readOnly: true
  # - name: shared-storage
  #   mountPath: "/shared"           # Shared data directory
  #   readOnly: false
[]
volumeMounts:


  # @schema
  # arch:
  #   type: string
  #   enum: ["amd64", "arm64"]
  # @schema
  # -- Target CPU architecture for container scheduling and node selection
  # Ensures pods run on compatible nodes (amd64 for Intel/AMD, arm64 for ARM processors)
[]
arch: amd64

# @schema
# additionalProperties: true
# @schema
# -- Pod Disruption Budget to maintain availability during cluster maintenance and updates
podDisruptionBudget:

  # -- Enable Pod Disruption Budget
  enabled:

    # -- Maximum pods that can be unavailable during voluntary disruptions (node drains, updates)
    false
  maxUnavailable:
    # Alternative: minAvailable: 1  # Minimum pods that must remain available
    # @schema
    # additionalProperties: true
    # @schema
    # -- Node selector labels for scheduling pods on specific nodes with required characteristics
    # Use for dedicated nodes, specific hardware, or compliance requirements
    # Examples:
    # dedicated: base-chart
    # Dedicated node pool
    # node-type: compute-optimized       # High-performance nodes
    # zone: us-east-1a                   # Specific availability zone
    1
nodeSelector:


  # @schema
  # additionalProperties: true
  # type: array
  # @schema
  # -- Tolerations allow pods to schedule on nodes with matching taints
  # Used for dedicated nodes, special hardware, or workload isolation
{}
tolerations:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Advanced pod scheduling rules for controlling pod placement relative to other pods and nodes
  # Provides fine-grained control over where pods are scheduled in the cluster for performance, compliance, and availability requirements.
  # Use cases:
  # - Co-location: Schedule pods near each other (e.g., app with cache)
  # - Anti-affinity: Spread pods across nodes/zones for high availability
  # - Node affinity: Target specific hardware, zones, or node types
  # - Mixed workload isolation: Separate different application tiers
  # Example configurations:
  # # Pod Anti-Affinity: Spread pods across different nodes for high availability
  # podAntiAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       podAffinityTerm:
  #         labelSelector:
  #           matchExpressions:
  #             - key: app.kubernetes.io/name
  #               operator: In
  #               values:
  #                 - base-chart
  #         topologyKey: kubernetes.io/hostname
  #
  # # Zone Anti-Affinity: Distribute pods across availability zones
  # podAntiAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 80
  #       podAffinityTerm:
  #         labelSelector:
  #           matchLabels:
  #             app.kubernetes.io/name: base-chart
  #         topologyKey: topology.kubernetes.io/zone
  #
  # # Node Affinity: Prefer compute-optimized nodes for CPU-intensive workloads
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       preference:
  #         matchExpressions:
  #           - key: node-type
  #             operator: In
  #             values:
  #               - compute-optimized
  #               - cpu-optimized
  #
  # # Required Node Affinity: Must run on nodes with SSD storage
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #       - matchExpressions:
  #           - key: storage-type
  #             operator: In
  #             values:
  #               - ssd
  #
  # # Pod Affinity: Co-locate with Redis cache for low latency
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 100
  #       podAffinityTerm:
  #         labelSelector:
  #           matchLabels:
  #             app: redis
  #         topologyKey: kubernetes.io/hostname
[]
affinity:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Pod affinity configuration rules
  # - enabled: true
  #   weight: 100
  #   topologyKey: "kubernetes.io/hostname"
  #   excludeStable: false # Set to true if you want anti-affinity between canary and stable
  #   # For high traffic scenarios
  #   highTrafficMode:
  #     enabled: false
  #     weight: 50
  # - enabled: true
  #   weight: 80
  #   excludeStable: false # Set to true if you want anti-affinity between canary and stable
  #   topologyKey: "topology.kubernetes.io/zone"
  #   # For high traffic scenarios
  #   highTrafficMode:
  #     enabled: false
  #     weight: 30
{}
podAffinity:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Topology spread constraints for pod distribution
  # - enabled: true
  #   maxSkew: 1
  #   topologyKey: "topology.kubernetes.io/zone"
  #   whenUnsatisfiable: "ScheduleAnyway"
  #   # For high traffic scenarios
  #   highTrafficMode:
  #     enabled: false # Can be toggled via Helm values override
[]
topologySpreadConstraints:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Environment variables for runtime configuration and JVM tuning
  # -- JVM configuration for memory management, garbage collection, and security
  # Memory settings: heap (128m initial and max)
  # Security: disable log4j lookups to prevent log4shell vulnerability
[]
envVars:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Environment variables loaded from ConfigMaps for non-sensitive configuration
  # Useful for feature flags, API endpoints, logging levels, and operational settings
  # Example configuration:
  # LOG_LEVEL: INFO                     # Application logging verbosity
  # DEBUG_MODE: "false"                 # Enable debug features
  # API_TIMEOUT: "30s"                  # External API timeout
  # CACHE_TTL: "300"                    # Cache time-to-live in seconds
{JAVA_OPTS: '-XX:NativeMemoryTracking=summary -Xms100m -Xmx100m -Dthumbnailator.conserveMemoryWorkaround=true -Dlog4j2.formatMsgNoLookups=True'}
configMapEnvVars:


  # @schema
  # additionalProperties: true
  # @schema
  # -- ConfigMap files for application configuration, properties, and settings
  # Files are mounted to /var/migrator/jetty/resources via projected volume (optional, read-only)
  # Combined with secrets in a single mount point for unified configuration access
{}
configMap:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Environment variables loaded from Secrets for sensitive configuration
  # Use for passwords, API keys, tokens, and other confidential data
  # Example sensitive configuration:
  # DATABASE_PASSWORD: postgres-password   # Database connection password
  # API_TOKEN: external-service-token      # Third-party API authentication
  # JWT_SECRET: signing-key                # Token signing secret
  # ENCRYPTION_KEY: data-encryption-key    # Data encryption key
{}
secretsEnvVars:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Kubernetes Secret data for storing sensitive information securely
  # Values are automatically base64 encoded and encrypted at rest
  # Files are mounted to /var/migrator/jetty/resources via projected volume (optional, read-only)
  # Combined with configMap in a single mount point for unified configuration access
  # Example sensitive data:
  # database-password: "supersecret123"    # Database credentials
  # api-key: "sk-1234567890abcdef"         # External API key
  # tls-cert: "-----BEGIN CERTIFICATE----" # TLS certificate
  # signing-key: "private-key-content"     # JWT signing key
{}
secrets:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Argo Rollout configuration for advanced deployment strategies
{application.conf: 
  auth.client_secret: viafoura
  livecomments.service.jwt.secret1: this is my secret
  livecomments.service.jwt.secret2: this is another secret
  livecomments.service.mysql.password: vf
  livecomments.service.mysql.ro.password: vf}
rollout:

  # -- Whether to enable Argo Rollouts
  enabled:

    # -- Number of old ReplicaSets to retain for rollback
    true
  revisionHistoryLimit:


    # @schema
    # additionalProperties: true
    # @schema
    # -- Rollout strategy configuration
    10
  strategy:


    # @schema
    # additionalProperties: true
    # blueGreen:
    # @schema
    # -- Blue-Green deployment strategy
    blueGreen:

      # -- Whether to enable blue-green deployments
      enabled:

        # -- Whether to automatically promote new versions
        false
      autoPromotionEnabled:

        # -- Seconds to wait before auto-promotion
        false
      autoPromotionSeconds:

        # -- Number of preview replicas (null uses main replica count)
        30
      previewReplicaCount:

        # -- Seconds to wait before scaling down old version
        scaleDownDelaySeconds:

          # -- Canary deployment strategy
          # @schema
          # additionalProperties: true
          # canary:
          # @schema
          30
    canary:

      # -- Whether to enable canary deployments
      enabled:


        # @schema
        # additionalProperties: true
        # trafficRouting:
        # @schema
        # -- Istio traffic routing configuration for canary
        true
      trafficRouting:


        # @schema
        # additionalProperties: true
        # istio:
        # @schema
        istio:


          # @schema
          # additionalProperties: true
          # virtualService:
          # @schema
          virtualService:
            name: ""  # Will be set via tpl in rollout template
            routes:
              - primary
          destinationRule:
            name: ""  # Will be set via tpl in rollout template
            canarySubsetName: canary
            stableSubsetName:


              # @schema
              # additionalProperties: true
              # managedRoutes:
              # @schema
              # -- Routes managed by the rollout controller
              stable
      managedRoutes:


        # @schema
        # additionalProperties: true
        # steps:
        # @schema
        # -- Canary deployment steps with traffic weights and pauses
[]
      steps:
        -
          setWeight: 1
        -
          pause:
{}
        -
          setWeight: 5
        -
          pause:
{}
        -
          setWeight: 10
        -
          pause:
            duration: 10m
        -
          setWeight: 20
        -
          pause:
            duration: 10m
        -
          setWeight: 30
        -
          pause:
            duration: 10m
        -
          setWeight: 40
        -
          pause:
            duration: 10m
        -
          setWeight: 50
        -
          pause:
            duration: 10m
        -
          setWeight: 60
        -
          pause:
            duration: 10m
        -
          setWeight: 80
        -
          pause:
            duration: 10m
        -
          setWeight: 100
        -
          pause:
{}
      maxUnavailable:

        # -- Maximum number of surge pods during canary
        0
      maxSurge:


        # @schema
        # additionalProperties: true
        # @schema
        # -- Analysis configuration for automated rollout decisions
        10%
  analysis:

    # -- Whether to enable rollout analysis
    enabled:

      # -- Number of analysis runs to perform
      false
    count:

      # -- Number of failures before rollback
      3
    failureLimit:

      # -- Interval between analysis runs
      1
    interval:

      # -- Response time threshold in milliseconds
      1m
    responseTimeThreshold:

      # -- Step at which to start analysis
      500
    startingStep:

      # -- Success condition threshold (0.99 = 99% success rate)
      2
    successCondition:

      # -- Prometheus configuration for metrics collection
      0.99
    prometheus:

      # -- Prometheus server address
      address:

        # -- Analysis templates to use
        "http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090"
    templates:
      -
        templateName:


          # @schema
          # additionalProperties: false
          # @schema
          # -- Domain configuration for external access through Istio service mesh
          success-rate
hosts:


  # @schema
  # additionalProperties: true
  # @schema
  # -- Public domains accessible from the internet for client connections
  public:
    enabled: true

    # @schema
    # type: array
    # additionalProperties: true
    # @schema
    domains:
      - livecomments.example.org
  private:
    enabled:


      # @schema
      # type: array
      # additionalProperties: true
      # @schema
      # -- Internal/Private Domains not accessible from the internet for client connections
      false
    domains:


      # @schema
      # additionalProperties: true
      # @schema
      # -- Istio service mesh configuration
[]
istio:

  # -- Whether to enable Istio service mesh
  enabled:

    # -- Global Istio configurations
    true
  globals:

    # -- Global annotations for Istio resources
    annotations:

      # -- Ambient Mode configuration for ztunnel and waypoint proxies
{}
  ambient:

    # -- Whether to enable Ambient Mode (ztunnel mesh)
    enabled:

      # -- Namespace labels for ambient mode enrollment
      true
    namespaceLabels:
      istio.io/dataplane-mode:

        # -- Waypoint proxy configuration for L7 features
        ambient
    waypoint:

      # -- Whether to create a waypoint proxy for this service
      enabled:

        # -- Traffic type for waypoint (service or workload)
        false
      trafficType:

        # -- Sidecar Mode configuration for envoy proxy injection
        service
  sidecar:

    # -- Whether to enable Sidecar Mode (envoy proxy injection)
    enabled:

      # -- Sidecar injection configuration
      false
    injection:

      # -- Sidecar injection mode (auto, enabled, disabled)
      mode:

        # -- Pod-level sidecar injection annotation
        auto
      podAnnotation:

        # -- Namespace-level sidecar injection label
        sidecar.istio.io/inject
      namespaceLabel:

        # -- Sidecar proxy configuration
        istio-injection
    proxy:

      # -- Custom proxy configuration
      config:

        # -- Resource limits for sidecar proxy
{}
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 200m
          memory:

            # -- Sidecar proxy image (uses Istio default if empty)
            256Mi
      image:

        # -- Sidecar proxy log level
        ""
      logLevel:

        # -- Certificate Manager integration
        warning
  certManager:

    # -- Whether to enable cert-manager for TLS certificates
    enabled:

      # -- Certificate issuer to use
      false
    issuer:

      # -- Istio Gateway configuration
      "letsencrypt-prod"
  gateway:

    # -- Whether to create an Istio Gateway
    create:

      # -- Gateway name (generated if empty)
      true
    name:

      # -- Gateway annotations
      ""
    annotations:

      # -- Gateway Labels
{}
    labels:
      external-dns:

        # -- Gateway selector
        "true"
    selector:
      istio:

        # -- Private Gateway selector to exposure service for private traffic
        "ingressgateway"
    private:
      selector:
        app:

          # -- Public Gateway selector to exposure service for public traffic
          "istio-private-gateway"
    public:
      selector:
        app:

          # -- Gateway server configurations
          "istio-public-gateway"
    servers:
      -
        # -- Whether this server configuration is enabled
        enabled:

          # -- Port configurations
          true
        ports:
          -
            number: 80
            name: http
            protocol:

              # -- TLS configuration
              HTTP
        tls:

          # -- Whether TLS is enabled
          enabled:

            # -- TLS configuration options
            true
          config:

            # -- Whether to redirect HTTP to HTTPS
            httpsRedirect:

              # -- Whether this server configuration is enabled
              true
      -
        enabled:

          # -- Port configurations for HTTPS
          true
        ports:
          -
            number: 443
            name: https
            protocol:

              # -- TLS configuration for HTTPS
              HTTP
        tls:

          # -- Whether TLS is enabled
          enabled:

            # -- TLS configuration options
            true
          config:

            # -- TLS mode (AUTO_PASSTHROUGH, SIMPLE, etc.)
            mode:

              # -- Whether this server configuration is enabled
              AUTO_PASSTHROUGH
      -
        enabled:

          # -- Port configurations
          false
        ports:
          -
            number: 80
            name: tcpsocket
            protocol:

              # -- TLS configuration
              HTTP
        tls:

          # -- Whether TLS is enabled
          enabled:

            # -- TLS configuration options
            true
          config:

            # -- Whether to redirect HTTP to HTTPS
            httpsRedirect:

              # -- Additional gateway configuration options
              true
    additionalConfig:

      # -- Istio VirtualService configuration
{}
  virtualService:

    # -- Host for the VirtualService (defaults to service name)
    host:

      # -- Gateways to attach to the VirtualService
      ""
    gateways:

      # -- Additional route configuration
[]
    additionalRouteConfig:

      # -- Additional HTTP routes
{}
    additionalHttp:

      # -- Istio DestinationRule configuration
[]
  destinationRule:

    # -- Traffic policy for load balancing and connection pooling
    trafficPolicy:
      enabled: false
      config:
        connectionPool:
          tcp:
            maxConnections: 100
            connectTimeout: 30s
            tcpKeepalive:
              time: 7200s
              interval: 75s
          http:
            http1MaxPendingRequests: 64
            http2MaxRequests: 1000
            maxRequestsPerConnection: 10
            maxRetries: 3
        loadBalancer:
          simple: LEAST_CONN
        outlierDetection:
          consecutiveGatewayErrors: 5
          consecutive5xxErrors: 5
          interval: 30s
          baseEjectionTime: 30s
          maxEjectionPercent:

            # -- Fault injection configuration for testing
            50
  faultInjection:

    # -- Whether to enable fault injection
    enabled:

      # -- Fault injection configurations
      false
    configs:


      # @schema
      # additionalProperties: true
      # @schema
      # -- ServiceMonitor configuration for Prometheus metrics collection
{}
serviceMonitor:

  # -- Whether to enable ServiceMonitor for Prometheus scraping
  enabled:

    # -- Namespace where ServiceMonitor should be deployed (if different from app namespace)
    true
  namespace:

    # -- Prometheus release name for ServiceMonitor
    ""
  prometheusReleaseName:

    # -- Scraping interval for metrics
    kube-prometheus-stack
  interval:

    # -- Scraping timeout for metrics (should be less than interval)
    10s
  scrapeTimeout:

    # -- Metrics endpoint path
    5s
  path:


    # @schema
    # additionalProperties: true
    # labels:
    # @schema
    # -- Additional labels for ServiceMonitor
    /metrics
  labels:

    # -- Namespace selector configuration for cross-namespace monitoring
{}
  namespaceSelector:

    # -- Enable any namespace selector (allows monitoring across namespaces)
    any:

      # -- Specific namespaces to monitor (leave empty for current namespace only)
      false
    matchNames:


      # @schema
      # additionalProperties: false
      # @schema
      # -- Datadog configuration
      # Alternative configuration for collecting all metrics
      # additionalProperties: true
      # datadog:
      #   enabled: true
      #   containerName: "base-chart"
      #   namespace: "base-chart"
      #   metricsPath: "/metrics"
      #   collectAllMetrics: true
      #   ignoreMetrics:
      #     - "go_.*"
      #     - "process_.*"
      #     - "prometheus_.*"
      #   tags:
      #     - "env:production"
      #     - "service:base-chart"
[]
datadog:

  # -- Enable/disable Datadog monitoring
  enabled:

    # -- Container name (should match your main container name)
    true
  containerName:

    # -- Namespace prefix for metrics in Datadog
    "base-chart"
  namespace:

    # -- Metrics endpoint path
    "base-chart"
  metricsPath:

    # -- Collect all metrics (if true, ignores specific metrics list)
    "/metrics"
  collectAllMetrics:

    # -- Timeout for metric collection
    true
  timeout:

    # -- Enable health service check
    20
  healthServiceCheck:

    # -- Send histogram buckets
    true
  sendHistogramsBuckets:

    # -- Collect histogram buckets
    true
  collectHistogramBuckets:

    # -- Specific metrics to collect with transformations
    # Can be either strings (collect as-is) or objects (rename)
    true
  metrics:
    -
      # Collect these metrics as-is
      "http_requests_total"
    - "http_request_duration_seconds"
    - "database_connections_active"
    -
      source: "go_memstats_alloc_bytes"
      target: "memory_allocated"
    -
      source: "process_cpu_seconds_total"
      target: "cpu_usage_seconds"
    -
      source: "base_custom_metric_total"
      target: "custom_operations"
    -
      source: "prometheus_rule_evaluation_duration_seconds"
      target:

        # -- Metrics to ignore/exclude
        "rule_evaluation_time"
  ignoreMetrics:
    - "go_gc_.*"
    - "go_goroutines"
    - "process_.*_bytes"
    - "prometheus_.*"
    - "up"
  metricTransformations:
    labels:
      -
        from: "handler"
        to: "endpoint"
      -
        from: "method"
        to: "http_method"
      -
        from: "status_code"
        to:

          # -- Additional tags to add to all metrics
          # Note: version tag will be automatically replaced with the actual app version from Chart.yaml or image.tag
          "response_code"
  tags:
    env: production
    team: backend
    service: base-chart
  logs:
    enabled: true
    source: "base-chart"
    service:
      # Log processing rules
      "base-chart"
    logProcessingRules:
      -
        type: "exclude_at_match"
        name: "exclude_health_checks"
        pattern: "GET /health"
      -
        type: "mask_sequences"
        name: "mask_credit_cards"
        pattern: "\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}"
        replacement:

          # -- JMX (Java Management Extensions) integration configuration
          # Complete configuration for JMX monitoring with Datadog Agent
          "****-****-****-****"
  jmx:

    # -- JMX connection URL (auto-generated if not provided)
    # Example: "service:jmx:rmi:///jndi/rmi://%%host%%:9016/jmxrmi"
    jmxUrl:

      # -- JMX authentication username
      ""
    user:

      # -- JMX authentication password (store in secret for production)
      ""
    password:

      # -- Whether this is a JMX integration (default: true when jmx integration is selected)
      ""
    isJmx:

      # -- Collect default JMX metrics (heap, threads, GC, etc.)
      true
    collectDefaultMetrics:

      # -- Process name regex for JMX connection discovery
      # Example: ".*java.*base-chart.*"
      true
    processNameRegex:

      # -- Custom tools.jar path for JMX tools
      ""
    tools_jar_path:

      # -- Custom name identifier for this JMX instance
      ""
    name:

      # -- Custom Java binary path
      ""
    java_bin_path:

      # -- JVM options for JMX connection
      # Example: "-Xms64m -Xmx128m"
      ""
    java_options:

      # -- SSL/TLS trust store path for secure JMX connections
      ""
    trust_store_path:

      # -- SSL/TLS trust store password
      ""
    trust_store_password:

      # -- SSL/TLS key store path for client certificates
      ""
    key_store_path:

      # -- SSL/TLS key store password
      ""
    key_store_password:

      # -- Enable SSL for RMI registry connections
      ""
    rmi_registry_ssl:

      # -- RMI connection timeout in milliseconds
      false
    rmi_connection_timeout:

      # -- RMI client timeout in milliseconds
      20000
    rmi_client_timeout:

      # -- Collect default JVM metrics (memory, GC, threads)
      15000
    collect_default_jvm_metrics:

      # -- Enable new garbage collection metrics format
      true
    new_gc_metrics:

      # -- Custom service check prefix for JMX health checks
      true
    service_check_prefix:

      # -- Custom metric configuration for JMX beans
      # Define specific MBeans and attributes to collect
      "jmx"
    conf:
      -
        # Example JMX metric collection configuration
        include:
          domain: "java.lang"
          type: "Memory"
          attribute:
            HeapMemoryUsage:
              alias: "jvm.heap_memory"
              metric_type: "gauge"
            NonHeapMemoryUsage:
              alias: "jvm.non_heap_memory"
              metric_type: "gauge"
      -
        include:
          domain: "java.lang"
          type: "GarbageCollector"
          name: "*"
          attribute:
            CollectionCount:
              alias: "jvm.gc.cms.count"
              metric_type: "counter"
            CollectionTime:
              alias: "jvm.gc.cms.time"
              metric_type: "counter"
      -
        include:
          domain: "java.lang"
          type: "Threading"
          attribute:
            ThreadCount:
              alias: "jvm.thread_count"
              metric_type: "gauge"
            DaemonThreadCount:
              alias: "jvm.daemon_thread_count"
              metric_type:
                # Application-specific MBeans (customize for your application)
                "gauge"
      -
        include:
          domain: "com.your.app"
          type: "ConnectionPool"
          attribute:
            ActiveConnections:
              alias: "app.db.active_connections"
              metric_type: "gauge"
            IdleConnections:
              alias: "app.db.idle_connections"
              metric_type:

                # -- Custom integration configuration
                # For integrations other than openmetrics or jmx
                "gauge"
  instanceConfig:
    # Example custom integration:
    # host: "%%host%%"
    # port: 8080
    # username: "monitor"
    # password: "secret"
    # ssl: true
    # timeout: 30
    # -- Integration type: openmetrics, jmx, or custom
    # openmetrics: For Prometheus-style metrics endpoints
    # jmx: For Java Management Extensions monitoring
    # custom: For other monitoring integrations
{}
  integration:

    # -- Custom init configuration for the integration
    # Additional configuration passed to the integration's init_config
    "jmx"
  initConfig:

    # -- APM (Application Performance Monitoring) configuration
{}
  apm:
    enabled: false
    serviceName: "base-chart"
    environment:


      # @schema
      # additionalProperties: true
      # @schema
      # -- NetworkPolicy configuration for defense-in-depth network security
      # Implements zero-trust networking with explicit allow rules for required communication
      "production"
networkPolicy:

  # -- Enable NetworkPolicy creation for enhanced network security
  enabled:

    # -- Deny-all baseline policy configuration
    false
  denyAll:

    # -- Create a deny-all NetworkPolicy as security baseline (recommended for production)
    enabled:

      # -- Ingress traffic rules configuration
      false
  ingress:

    # -- Allow ingress from pods within the same namespace
    allowSameNamespace:

      # -- Allow ingress from Istio Gateway for service mesh traffic
      true
    allowIstioGateway:

      # -- Allow ingress from monitoring namespace for metrics collection
      true
    allowMonitoring:

      # -- Name of the monitoring namespace (default: monitoring)
      true
    monitoringNamespace:

      # -- Custom ingress rules for specific traffic requirements
      "monitoring"
    customRules:
      # Example custom rules:
      # - namespaceSelector:
      #     matchLabels:
      #       name: "api-gateway"
      #   ports:
      #     - protocol: TCP
      #       port: 8080
      # - ipBlock:
      #     cidr: "10.0.0.0/8"
      #   ports:
      #     - protocol: TCP
      #       port: 8080
      # -- Egress traffic rules configuration
[]
  egress:

    # -- Enable egress policies (when false, all egress is allowed)
    enabled:

      # -- Allow DNS resolution (required for service discovery)
      false
    allowDNS:

      # -- Allow HTTPS traffic to internet (for external APIs, image pulls)
      true
    allowHTTPS:

      # -- Allow traffic within the same namespace
      true
    allowSameNamespace:

      # -- Allow access to AWS metadata service (required for AWS integrations)
      true
    allowAWSMetadata:

      # -- Custom egress rules for specific external services
      true
    customRules:
      # Example custom rules:
      # - namespaceSelector:
      #     matchLabels:
      #       name: "database"
      #   ports:
      #     - protocol: TCP
      #       port: 5432
      # - ipBlock:
      #     cidr: "192.168.1.0/24"
      #   ports:
      #     - protocol: TCP
      #       port: 443
      # -- Monitoring-specific NetworkPolicy configuration
[]
  monitoring:

    # -- Create dedicated NetworkPolicy for monitoring access
    enabled:

      # -- Prometheus namespace for metrics scraping
      true
    prometheusNamespace:

      # -- Datadog namespace for agent access
      "monitoring"
    datadogNamespace:


      # @schema
      # additionalProperties: true
      # @schema
      # -- Pod Security Standards configuration for compliance automation
      # Implements Kubernetes Pod Security Standards for enhanced security posture
      "datadog"
podSecurityStandards:

  # -- Enable Pod Security Standards compliance features
  enabled:

    # -- Pod Security Standards level: privileged, baseline, or restricted
    # restricted: Most secure, suitable for security-critical applications
    # baseline: Minimally restrictive, prevents known privilege escalations
    # privileged: Unrestricted, suitable for system-level workloads
    true
  level:

    # -- Create PodSecurityPolicy for clusters without Pod Security Standards
    # (Kubernetes < 1.25 or clusters still using PSP)
    "restricted"
  createPodSecurityPolicy:

    # -- Create SecurityContextConstraints for OpenShift clusters
    false
  createSecurityContextConstraints:

    # -- Allow hostPath volumes (not recommended for restricted level)
    false
  allowHostPaths:

    # -- Allowed hostPath configurations (only if allowHostPaths is true)
    false
  allowedHostPaths:
    # Example:
    # - pathPrefix: "/tmp"
    #   readOnly: true
    # - pathPrefix: "/var/log"
    #   readOnly: false
    # -- Allowed host port ranges (empty means no host ports allowed)
[]
  allowedHostPortRanges:
    # Example:
    # - min: 8000
    #   max: 8080
    # -- Allowed Linux capabilities (empty for restricted level)
[]
  allowedCapabilities:
    # Example (not recommended for restricted):
    # - "NET_BIND_SERVICE"
    # - "SYS_TIME"
    # -- Allowed seccomp profiles beyond RuntimeDefault
[]
  allowedSeccompProfiles:
    # Example:
    # - type: "Localhost"
    #   localhostProfile: "my-profile.json"
    # -- AppArmor configuration
[]
  apparmor:

    # -- Enable AppArmor profiles
    enabled:

      # -- AppArmor profiles for containers
      false
    profiles:
      # Example:
      # base-chart: "runtime/default"
      # sidecar: "localhost/my-profile"
      # -- Compliance and audit settings
{}
  compliance:

    # -- Enable compliance monitoring and reporting
    enabled:

      # -- Compliance frameworks to validate against
      true
    frameworks:
      - "pod-security-standards"
      - "cis-kubernetes"
      - "nist-cybersecurity"
    auditLogging:
      enabled:

        # -- Violation severity levels to log
        true
      levels:
        - "warn"
        -
          # @schema
          # additionalProperties: true
          # @schema
          # -- Backup automation configuration for operational excellence
          # Provides multiple backup strategies including Velero and custom CronJob backups
          "error"
backup:

  # -- Enable backup automation features
  enabled:

    # -- Velero backup configuration for full cluster backup
    false
  velero:

    # -- Enable Velero backup scheduling
    enabled:

      # -- Backup schedule in cron format (default: daily at 2 AM)
      false
    schedule:

      # -- Backup retention period (default: 30 days)
      "0 2 * * *"
    retention:

      # -- Velero namespace (default: velero)
      "720h"
    namespace:

      # -- Storage location for backups
      "velero"
    storageLocation:

      # -- Volume snapshot locations for persistent volumes
      "default"
    volumeSnapshotLocations:
      - "default"
    includeCustomResources:

      # -- Custom resources to include in backup
      true
    customResources:
      - "rollouts.argoproj.io"
      - "virtualservices.networking.istio.io"
      - "destinationrules.networking.istio.io"
      - "gateways.networking.istio.io"
    hooks:
      enabled:

        # -- Custom CronJob backup for application-specific data
        true
  cronjob:

    # -- Enable CronJob-based backup
    enabled:

      # -- Backup schedule in cron format (default: daily at 3 AM)
      false
    schedule:

      # -- Timezone for backup schedule
      "0 3 * * *"
    timeZone:

      # -- Concurrency policy for backup jobs
      "UTC"
    concurrencyPolicy:

      # -- Number of successful backup jobs to retain
      "Forbid"
    successfulJobsHistoryLimit:

      # -- Number of failed backup jobs to retain
      3
    failedJobsHistoryLimit:

      # -- Deadline for backup job to start
      1
    startingDeadlineSeconds:

      # -- Number of retries for failed backup jobs
      300
    backoffLimit:

      # -- Maximum time for backup job to complete
      2
    activeDeadlineSeconds:

      # -- Retention period for backups (in days)
      3600
    retentionDays:

      # -- Type of backup to perform
      30
    backupType:

      # -- Container image for backup executor
      "application-data"
    image:
      repository: "amazon/aws-cli"
      tag: "2.13.0"
      pullPolicy:

        # -- Resource limits and requests for backup job
        "IfNotPresent"
    resources:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory:

          # -- S3 configuration for backup storage
          "128Mi"
    s3:

      # -- S3 bucket for backup storage
      bucket:

        # -- S3 prefix for backup objects
        ""
      prefix:

        # -- Additional environment variables for backup job
        "backups"
    extraEnv:
      # Example:
      # AWS_REGION: "us-east-1"
      # BACKUP_ENCRYPTION: "true"
      # -- Additional volumes for backup job
{}
    volumes:
      # Example:
      # - name: config
      #   configMap:
      #     name: backup-config
      # -- Additional volume mounts for backup job
[]
    volumeMounts:
      # Example:
      # - name: config
      #   mountPath: /etc/backup
      #   readOnly: true
      # -- Node selector for backup job placement
[]
    nodeSelector:
      # Example:
      # kubernetes.io/arch: amd64
      # node-role.kubernetes.io/worker: "true"
      # -- Tolerations for backup job scheduling
{}
    tolerations:
      # Example:
      # - key: "backup-node"
      #   operator: "Equal"
      #   value: "true"
      #   effect: "NoSchedule"
      # -- Monitoring configuration for backup operations
[]
  monitoring:

    # -- Enable backup monitoring and metrics
    enabled:

      # -- Prometheus metrics for backup status
      true
    metrics:

      # -- Enable backup metrics collection
      enabled:

        # -- Metrics endpoint path
        true
      path:

        # -- Metrics collection interval
        "/metrics"
      interval:

        # -- Alerting configuration for backup failures
        "30s"
    alerting:

      # -- Enable backup failure alerts
      enabled:

        # -- Alert severity levels
        true
      severity:

        # -- Alert notification channels
        "warning"
      channels:
        - "slack"
        -
          # @schema
          # additionalProperties: true
          # @schema
          # -- Grafana Dashboards Configuration
          "email"
grafana:

  # -- Dashboard configuration
  dashboards:

    # -- Enable Grafana dashboard ConfigMap creation
    # Only creates ConfigMap if dashboard file exists at dashboards/base-chart.json
    enabled:

      # -- Grafana folder for dashboard organization
      false
    folder:

      # -- Additional labels for dashboard ConfigMap discovery
      "Base-Chart"
    labels:
      grafanaDashboard: "1"
      dashboardSource:


        # @schema
        # additionalProperties: true
        # type: object
        # @schema
        # -- Grafana Dashboard Annotations
        # Example annotations:
        # grafana.com/dashboard-uid: "base-chart-dashboard"
        # grafana.com/dashboard-title: "Base-Chart Service Monitoring"
        "base-chart"
    annotations:
{}
configmap:
  application.properties:
    auth.client_uuid: 0d0b1d94-4971-43ef-bdaa-8108ac1ffc55
    auth.refresh_token_backoff_multiplier: 1
    auth.refresh_token_interval_ms: 270000
    auth.refresh_token_max_attempts: 3
    auth.refresh_token_wait_duration_ms: 1000
    auth.service_uri: "https://auth/authorize_client"
    compliance.export.bucket: viafoura-gdpr-exports.misconfigured
    compliance.kafka.bootstrap: "kafka:9092"
    content_scores.kafka.bootstrap: "kafka:9092"
    customer.keys.get_retry_count: 30
    customer.keys.get_retry_wait_ms: 1000
    livecomments-trending.service.max_retries: 10
    livecomments-trending.service.mysql.db: livecomments
    livecomments-trending.service.mysql.ro.host: livecomments-mysql-trending
    livecomments-trending.service.mysql.ro.idle_timeout_ms: 60000
    livecomments-trending.service.mysql.ro.max_lifetime_ms: 600000
    livecomments-trending.service.mysql.ro.max_pool_size: 5
    livecomments-trending.service.mysql.ro.min_pool_size: 1
    livecomments-trending.service.mysql.ro.password: vf
    livecomments-trending.service.mysql.ro.user: livecomments
    livecomments.service.aws.bucket_name: origin-livecommentsmedia.env.net
    livecomments.service.cache.parent_site_cache_ttl_m: 10
    livecomments.service.cache.parent_site_max_cache_size: 1000
    livecomments.service.cache.relative_sections_cache_ttl_m: 5
    livecomments.service.cache.relative_sections_max_cache_size: 300
    livecomments.service.cache.site_group_cache_ttl_m: 5
    livecomments.service.cache.site_group_max_cache_size: 100
    livecomments.service.content_container_cache_max_size: 5000
    livecomments.service.content_container_cache_ttl_m: 5
    livecomments.service.content_container_comment_count_and_status_cache_max_size: 5000
    livecomments.service.content_container_comment_count_and_status_cache_ttl_m: 5
    livecomments.service.health-check.liveness.port: 8081
    livecomments.service.heartbeat_interval: 30000
    livecomments.service.max_content_page_size: 100
    livecomments.service.mysql.connection_timeout_ms: 10000
    livecomments.service.mysql.db: livecomments
    livecomments.service.mysql.host: livecomments-mysql
    livecomments.service.mysql.idle_timeout_ms: 60000
    livecomments.service.mysql.max_lifetime_ms: 60000
    livecomments.service.mysql.max_pool_size: 5
    livecomments.service.mysql.max_retries: 10
    livecomments.service.mysql.min_pool_size: 1
    livecomments.service.mysql.ro.connection_timeout_ms: 10000
    livecomments.service.mysql.ro.host: livecomments-mysql-ro
    livecomments.service.mysql.ro.idle_timeout_ms: 60000
    livecomments.service.mysql.ro.max_lifetime_ms: 600000
    livecomments.service.mysql.ro.max_pool_size: 5
    livecomments.service.mysql.ro.min_pool_size: 1
    livecomments.service.mysql.ro.user: livecomments
    livecomments.service.mysql.user: livecomments
    livecomments.service.request.timeout: 20000
    livecomments.service.server.port: 8080
    livecomments.service.settings_cache_max_size: 5000
    livecomments.service.settings_cache_ttl_m: 5
    livecomments.service.total_visible_content_cache_max_size: 2000
    livecomments.service.total_visible_content_cache_ttl_m: 5
    livecomments.service.trending_content_cache_max_size: 500
    livecomments.service.user_profile_cache_max_size: 200
    livecomments.service.user_profile_cache_ttl_m: 5
    moderation.kafka.bootstrap: "kafka:9092"
    publisher.kafka.bootstrap: "kafka:9092"
    section_tree.client.cache_expiry_sec: 15
    section_tree.client.service_url: "http://tyrion/v3/sections"
    section_tree.client.timeout_ms: 5000
    settings_client.cache_expiry_ms: 1
    settings_service.request_timeout_ms: 5000
    settings_service.url: "http://tyrion/v3/settings"
    user_actions.kafka.bootstrap: "kafka:9092"
    v2api.uri: "http://phpapi/v2"
    validation.originUrl.definedDomainCheck.enabled: true
  vfmetrics.properties:
    datadog.pattern: ".*"
    datadog.prefix: ''
    datadog.report-interval: 60
    datadog.tags: "environment:misconfigured"
    datadog.transport.type: udp
    heartbeat.enabled: true
    heartbeat.report-interval: 1
    namespace: livecomments
    reporters: ''